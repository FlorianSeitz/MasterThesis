\documentclass[a4paper,man,natbib]{apa6}

\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage{graphicx}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage{amsmath,amsfonts,mathabx}
\usepackage{epigraph}

%\setmainfont{Times New Roman} 
%\setsansfont{Times New Roman} 
%\setmonofont{Times New Roman} 
% Keywords command

\providecommand{\keywords}[1]
{
  \textbf{\textit{Keywords---}} #1
}
\usepackage[doublespacing]{setspace}

\setlength\epigraphwidth{.8\textwidth}

\title{The Discrete Metric in Categorization Under Time Pressure}
% Effect of Time Pressure on Distance Metrics in Categorization
\shorttitle{Discrete Metric Under Time Pressure}
\author{Florian I. Seitz}
\affiliation{University of Basel}

\abstract{The generalized context model \citep{nosofsky1986attention}, which excels at explaining human categorization behavior, assumes the Minkowski metric to represent psychological distance. I propose that in categorization under time pressure people compute psychological distance heuristically with the discrete metric which counts the number of non-identical feature values. 
61 psychology students (43 females, $M_{age}$ = 24.13 years, age range: 19--50 years) from the University of Basel acquired in the learning phase the category structure of eight stimuli with three quarternary features by trial--by--trial supervised category learning. After meeting the accuracy criterion participants categorized six novel and four old stimuli 14 times without feedback in the test phase. We applied time pressure to a random half of participants in the test phase.
Inferential tests at the aggregate level showed that pressure the Minkowski metric could account for participants' categorization behavior both with and without time (\textit{p} < .001, respectively). Cognitive modeling yielded that the discrete metric with attention centered on all features in the condition without time pressure and a random choice model in the condition with time pressure outperformed the other models both on the aggregate and on the individual level.
The present findings suggest that the discrete may not apply to people in speeded categorization situations but to a substantial amount of people in categorization without time pressure. We compare these findings to a potential rule--based model and point out potential differences in cognitive processing when categorizing stimuli with known versus unknown feature values.} 

\keywords{categorization, time pressure, Minkowski metric, discrete metric, distance}

\begin{document}
\maketitle

%\vspace*{\fill}
\epigraph{There is nothing more basic than categorization to our thought, perception, action, and speech. [...] An understanding of how we categorize is central to any understanding of how we think and how we function, and therefore central to an understanding of what makes us human.}{George Lakoff, 1987, pp.5--6}

Categorization refers to the partitioning of a number of objects into a smaller amount of groups where each group is associated with a unique response \citep{cohen2005bridging, nosofsky1986attention, nosofsky1989further}. As people categorize countless times throughout their life, the mental process of classifying objects into categories is considered to be one of the most fundamental cognitive phenomena overall \citep{ashby2001categorization, bruner1956study, cohen2005bridging, lakoff1987women, goldstone2003concepts} and thus has received substantial attention in scientific research. From the late 70s on, numerous cognitive models have been established which try to describe how people perform categorizations \citep[for an overview over the diverse models of categorization, see][]{kruschke2008models,wills2013models}. The present thesis analyzes an especially popular categorization model---the generalized context model \citep{nosofsky1984choice, nosofsky1986attention, nosofsky2011generalized}. The generalized context model is defined as an exemplar--based model, where people retrieve instances from memory (i.e., exemplars) and compare them with an instance to categorize \citep[i.e., the probe;][]{medin1978context}. Comparisons between probe and exemplars are expressed as distances which in turn are used to predict category membership. Exemplar-based models of categorization may be distinguished from prototype-based models of categorization, which assume the retrieval of each category's central tendency instead of individual exemplars \citep{reed1972pattern, smith1997straight, smith1998prototypes}. 

In the following parts of this introduction, I will describe and review the generalized context model. Then, I will argue that people might use a binary measure of distance, the discrete metric, when confronted with time pressure during categorization. Differences between the generalized context model implemented with the usual distance metric and the one implemented with the discrete distance will be discussed before presenting the study which aimed to test against each other these two models in a categorization task with time pressure.

\subsection{The Generalized Context Model}
The generalized context model \citep{nosofsky1986attention} assumes three steps when people categorize a probe into one of two a priori defined categories: First, people retrieve exemplars from both categories and calculate distances between the probe and each retrieved exemplar. These distances can be interpreted as how far away the probe is spatially positioned from the different exemplars. Second, each distance is transformed into a measure of similarity, such that high distances correspond to low similarities and low distances to high similarities. Finally, the aggregate similarity of the probe to all exemplars of one category relative to the aggregate similarity to both categories states the model's prediction of classification probability. The higher the similarity of the probe to exemplars of one category the higher the probability of assigning the probe into this very category.

In the following, I will describe all three steps the generalized context model assumes in more detail. The formalization of the model is equivalent to \cite[][pp.281--282]{nosofsky1989further}, except that it has been generalized to more than two dimensions. 

\subsubsection{Distance}
The generalized context model \citep{nosofsky1984choice,nosofsky1986attention} assumes people to represent an exemplar as a point in a $M$--dimensional space where each feature of the exemplar is one dimension and the exemplar's feature value is the coordinate on the corresponding dimension. In order to assign probe $i$ into one of two categories, the generalized context model first calculates the Minkowski distance between probe $i$ and previously encountered exemplars from the categories which have been retrieved from memory. A Minkowski distance is a geometric distance function (also named metric), defined as: 
\begin{equation}
d_{ij} = \left[\sum\limits_{m=1}^M w_{m}*\mid x_{im} - x_{jm}\mid ^r\right]^\frac{1}{r},
\end{equation}
where $d_{ij}$ is the Minkowski distance between probe $i$ and exemplar $j$, $x_{im}$ is the value of probe $i$ on feature $m$, $w_{m}$ is the attention weight attributed to feature $m$ (with $0 \leq w_{m} \leq 1$ and $\sum w_{m} = 1$), $M$ is the number of features, and $r$ describes the form of the distance metric (with $r >= 1$). Popular instances of the Minkowski distance are the Manhattan distance ($r = 1$) for highly separable-feature stimuli and the Euclidean distance ($r = 2$, a generalized form of the Pythagorean theorem for $M$ features) for integral-feature stimuli \textit{cite Shepard 1964, Nosofsky, 1986, and Garner, 1974}. With increasing $r$ the resulting distance between a given probe and a given exemplar cannot increase and absolute differences between distances to different exemplars are reduced. Values of $r$ are constrained to be equal to or higher than 1 for adherence with the triangle inequality, a prerequisite of geometric distance metrics \citep{jakel2008similarity,francois2007concentration,tversky1982similarity,beals1968foundations}. The triangle inequality states that in each triangle the sum of two sides must be at least as great as the remaining side. For the Minkowski metric this means that the sum of all absolute differences between probe and exemplar must be greater than the resulting distance. This prerequisite is violated for $r < 1$ and in such a case the Minkowski distance is not anymore a metric \citep[][p. 5]{kress1989linear}. 

%To Do: Write that Minkowski is geometric (cite Goldstone for other similarity calculations) and then write below that geometric models with r < 1 don't fulfill the triangle inequality axiom.

\subsubsection{Similarity}
In a second step, the distance $d_{ij}$ is transformed into a measure of similarity by means of the function \citep{nosofsky1986attention}:
\begin{equation}
s_{ij} = \exp\left(-c*d_{ij}^p\right),
\end{equation}
where $s_{ij}$ is the similarity between probe $i$ and exemplar $j$, $c$ (with $0 \leq c$) is an overall sensitivity parameter, and $p$ is a parameter that determines how similarity relates to psychological distance. Popular instances of the similarity function are the exponential decay function ($p = 1$) for discriminable stimuli and the Gaussian function ($p = 2$) for confusable stimuli \textit{Ennis 1988, Ennis, Palen, & Mullen, 1989, Nosofsky, 1985}. The sensitivity parameter $c$ describes the convexity of the similarity function indicating thus the steepness with which similarity decreases as distance increases. For high values of $c$, similarities are already low at very small distances already. For low values of $c$, similarities are still high at very large distances. In both cases, the model doesn't discriminate between different distances. The parameter $c$ thus denotes a person's sensitivity to psychological distance.

\subsubsection{Categorization probability}
Finally, the probability with which probe $i$ is categorized into category $A$ is defines as 
\begin{equation}
P(R_{A}|i) = \frac{b_{A}\sum\limits_{j \in A} s_{ij}}{b_{A}\sum\limits_{j \in A} s_{ij} + (1 - b_{A})\sum\limits_{k \in B} s_{ik}},
\end{equation}
where $P(R_{A}|i$ is the probability of rendering response $A$ given probe $i$ and $b_{A}$ is the response bias for category $A$. The function is derived from the similarity--choice model for stimulus identification \textit{Luce, 1963, Shepard, 1957} and bases categorization probability on the share of the aggregate similarity to both categories that is attributable to one of the two categories.

\subsubsection{Delimitation from prototype models}
Because it assumes people to rely on actual instances experienced in the past to represent categories, the generalized context model is an exemplar--based model of categorization \citep{medin1978context, nosofsky1986attention}. The model diverges in this assumption from other existing approaches which assume that categories are represented by central tendencies (i.e., prototypes) which are retrieved and compared to the probe at hand \citep{nosofsky1987attention}. The main difference between exemplar-based and prototype-based categorization models thus occurs in the first step of the categorization procedure as different category representatives are being retrieved from memory (i.e., exemplars or prototypes). Due to the very fine--grained representation of categories using actual experienced instances, the categorization predictions of the generalized context model are sensitive to influences from individual exemplars---influences which are absent in prototype models \citep{nosofsky2011generalized, nosofsky1992exemplars, medin1978context}. Following a long debate of whether people represent categories using prototypes or exemplars, research indicates that exemplar-based categorization models using a non--linear similarity rule, such as the generalized context model, outperform prototype models in explaining participants' categorization behavior \citep{nosofsky2002exemplar, nosofsky1992exemplars}.

\subsection{Categorization under Time Pressure}

\subsection{Hypotheses}
Based on the reanalysis of \textit{Wills}, we state the following hypotheses: Under time pressure, people use the discrete metric (H1a), else a unidimensional Minkowski metric (H1b), else the multidimensional Minkowski metric (H1c). For the rank order of model fits, the multidimensional discrete metric performs equally well as the unidimensional discrete metric 

\section{Method}

\subsection{Optimal Experimental Design}

An optimal experimental design is defined as a design with ``the greatest likelihood of differentiating the models under consideration'' \cite[][p. 500]{myung2009optimal}. Being able to differentiate between the different models under consideration means that each model is associated with unique behavioral predictions. Participant behavior can thus be linked with greater certainty to one of the models under consideration. Furthermore, through the maximization of model prediction differences, design optimization increases model recovery (i.e., the model with which data has been generated is more often found to be the best-fitting model as well). The advantages of design optimization are thus two-fold: A participant's behavior can be associated more exclusively with one of the models under consideration and the odds are higher that this best-fitting model is the model that was used by the participant to generate her responses \citep[albeit that each scientific model is only an approximation of the participant's cognitive model; see][]{myung2009optimal}. Design optimization maximizes thus the experiment's degree of informativeness and cost-effectiveness by improving the design while keeping the necessary sample and trial size at a low level \citep{cavagnaro2009better, ouyang2016practical, raffert2012optimally, atkinson2007optimum, nelson2005finding}. 

The advantages of design optimization are most pronounced when the different models differ quantitatively instead of qualitatively and are thus more similar to each other as well as when several variables have to be considered simultaneously during the designing process of the experiment making thus a good design hardly visible to the naked eye \citep{myung2009optimal}. Both these criteria apply for categorization studies: First, categorization models with a Minkowski metric vs. a discrete metric differ in their predicted probabilities of assigning a given stimulus into one of two categories. Second, the experimental design includes many variables to consider, such as which stimuli to include in either learning or test set and within the learning set how the stimuli are assigned to the two categories. \cite{myung2009optimal} conducted a reanalysis of the designs of the first two experiments of \cite{smith1998prototypes} which aimed at distinguishing the GCM \citep{nosofsky1986attention} from the Multiplicative Prototype Model \citep{smith1998prototypes} by using six-dimensional binary stimuli. \cite{myung2009optimal} found that \citeauthor{smith1998prototypes}'s nonlinearly separable design had a higher model recovery than their linearly separable design (88.8\% vs. 72.9\%) and both performed better than a simple design where each category was almost exclusively associated with either one of the two dimension values and where model recovery was at a rate of 53.1\%. Still, the optimal design that was possible yielded a model recovery rate of 96.3\%, minimizing thus the risk of a model fitting error by eight percent compared to the nonlinearly separable design.  

In the light of these findings, we ran simulations to find the design that would best discriminate between the Generalized Context and the Multiplicative Prototype Model both implemented with the Minkowski and the discrete metric. The simulations were iterated for the following variables: i) the design (i.e., defining learning and test set as well as category structure within the learning set), ii) the true model (either the Generalized Context Model or the Multiplicative Prototype Model with either the Minkowski metric or the discrete metric), iii) the true parameter combination (i.e., attention weights \textit{w}'s ranging from $0$ to $1$ in steps of $1/3$ and the overall sensitivity parameter \textit{c} ranging from $0.1$ to $4.1$ in steps of $1$), and iv) the fitting model (either the Generalized Context Model or the Multiplicative Prototype Model with either the Minkowski metric or the discrete metric).

Given that our stimuli consisted of three dimensions with four values each, we had a total number of 64 stimuli out of which we wanted to have eight in the learning set (i.e., four in each category). Not counting exactly inversed and thus repetitive partitioning of learning stimuli into the two categories (e.g., stimuli 1--4 in category A and stimuli 5--8 in category B vs. stimuli 1--4 in category B and stimuli 5--8 in category A) this yields $\dfrac{64!}{56!*8!}*\dfrac{8!}{8!*4!*2} = 154,915,787,880$ possible designs. As this would lead the simulations to last very long, we restricted the design space to include only designs in which all stimuli had on each dimension maximally a city-block distance of 1 to each other. This restriction at the same time minimized the risk of biasing participants towards one of the models in the learning phase as the distances resulting from value ranges of 1 are the same for the Minkowski and the discrete metric and leads to a more approachable number of 140 possible designs.

For a given design, true model, and true parameter combination we simulated binary participant responses (i.e., category A or B) for the learning set which was replicated 20 times and for the test set which was replicated 10 times. All simulated responses were based on the predictions of the true model. We then fitted the free parameters (i.e., attention weights and the overall sensitivity parameter) for every model to the simulated data of the learning set excluding the first eight trials where models have not encountered enough instances to render informative predictions. We then calculated predictions for the test set with every model using the fitted parameters above and calculated the logarithmic likelihood between these predictions and the simulated participant responses for the test set under the true model. This data finally allowed us to check which model best fitted the data produced by the true model as thus ultimately to calculate the model recovery for a given design across all true parameter combinations.

We used optimal experimental design (Myung & Pitt, 2009) to find a categorization environment such that the two model versions (i.e., discrete metric and Minkowski metric) can be optimally discriminated given the responses during the test phase, in the expectation. This involved finding the best of all possible designs (that is, of all category structures for all possible 8 of 64 stimuli in the learning phase) under the constraint that all learning stimuli differed from each other maximally by 1 one each dimension. This constraint ensures that neither the discrete nor the Minkowski metric is favored by the learning phase design, because both metrics yield identical distances if feature values differ by maximally 1. Simulations were conducted over the following parameter ranges: attention weights ws ranging each from 0 to 1 and summing up to 1, varied in steps of $1/3$ and the sensitivity parameter c ranging from 0.1 to 4.1 in steps of 1. The decay parameter p and the parameter defining the distance metric r were fixed to 1. For each possible design and model parameter a model recovery was conducted. It involved comparing model performance on the simulated test phase data after estimating the free model parameters from the simulated learning phase data. We aimed to find the design for which the winning model recovered the true data-simulating model most often, across all possible model parameters.

\subsubsection{Model Recovery}
The generalized context model with the discrete metric was correctly recovered in 100\% of all cases, the generalized context model with the Minkowski metric was correctly recovered in 86.7\% \textbf{(I have to recheck this)} of all cases. 

\subsection{Materials}
The experiment was conducted on \textit{computers} and programmed in Python \textit{ref?}.  Participants classified different products (i.e. stimuli) into brand L or brand R (i.e., categories). Products were based on the material of \textit{Albrecht}. Each product consisted of the same three ingredients (i.e., features) represented by grey beams with one to four quantity units of each ingredient (i.e., feature values) represented by colored squares. Figure \textit{ref} illustrates the material. Participants learned the category structure of eight stimuli in the learning phase, and categorized four of them and six novel stimuli in the test phase. Figure \text{ref or table} shows the learning and test set environment. Brand name and the corresponding correct arrow key as well as the distribution of the features to the three grey beams and the three colors of the stimuli were randomized across participants.

\subsection{Design}
Participants assigned products to brand L and brand R by pressing the left and right arrow key, respectively. The dependent variable was thus the pressed key which represented the product's assumed brand. The independent variable was the time pressure induced to a random half of participants in the test phase. It allowed a participant 400 milliseconds plus 30\% of the median decision time needed by this participant across the final 100 learning trials. The study had thus a repeated--measures 2x1 between--subjects design. Further variables assessed were response times per trial and the amount of time pressure if any.

\subsection{Participants}
Sixty-one psychology students (43 females, $M_{age}$ = 24.13 years, $SD_{age}$ = 6.39 years, age range: 19--50 years) from the University of Basel were recruited over an online platform of the Faculty of Psychology to participate in a categorization study. All participants had a high--school or a higher degree and were asked for color blindness, visual impairment, and in case of impaired vision whether they carried a visual aid during the experiment. Participants selected themselves into the sample and there were no inclusion criteria. Up to six participants could show up to the same experimental session. Participant data were listwise deleted if the participant failed to reach the accuracy criterion in the learning phase within an hour, answered ''absolutely unclear'' or ''somewhat unclear'' to the question if the task in the experiment was clear or in the time pressure condition exceeded the time limit in more than 50\% of the test trials for a given test stimulus and in the no time pressure condition had a log transformed reaction time more than three standard deviations below the mean of the log transformed reaction times of the learning phase in more than 50\% of the test trials for a given test stimulus.  As a compensation for study participation participants received course credit proportionally to the amount of time needed to complete the study. Eight additional subjects participated in the study but had to be excluded from the analyses as they reported that the task was somewhat or absolutely unclear to them, two participants failed to reach the accuracy criterion, and two participants' data were used for pretesting purposes.

Sample size was predetermined by a model--based power simulation where the true probability of choosing the Minkowski metric changed from 70\% to 30\% and the true probability of choosing the discrete metric changed from 30\% to 70\% when time pressure was introduced. This resulted in a total sample size of N=60 (n=30 in each condition) to achieve 89\% power. Figure \textit{ref} illustrates. The final sample included 31 participants without time pressure and 30 participants with time pressure.

\subsection{Procedure}
After arriving at computer laboratory of the Center of Economic Psychology, participants were greeted, seated in front of a computer which was delimited from the other computers by dividing walls, and were provided with an informed consent which they could read through and sign in case of agreement. Upon signature, the experimenter typed in the participant id to start the experiment and the participant was presented a series of instructions on the computer (for the exact instructions, see \textit{appendix}. Participants first read, that they had to learn to assign different products to two different brands (brand L and brand R), that each product consisted of the same three ingredients but differed from other products in the specific ingredient quantities. They then saw a randomly chosen product (see Figure \textit{ref} for an example), read that each grey beam represented one ingredient and the number of colored squares in the beams indicated the quantity of the respective ingredient. To familiarize with the products, participants then had to click on each ingredient ten times. With each click the quantity of the respective ingredient increased or decreased by one unit, ensuring that all participants saw all possible values on each feature. Participants then read that in each trial they would be presented with a random chosen product and their task was to correctly guess the brand of the respective product by pressing the left and right arrow keys. Participants read that the experiment consisted of two phases: In the first phase, they would learn the correct brand of each product and receive feedback after entering their answer. After achieving to consistently assign the products to the correct brand, they would reach the second phase and have to assign again several products to the brand to which they belonged most likely without receiving feedback. Participants in the time pressure condition furthermore read that they would have a time limit for their response in the second phase which they shouldn't exceed.

\subsubsection{Learning phase}
In the learning phase participants learned the category structure of eight products. All blocks of the learning phase included each of the eight products; the sequence within each block was randomized. In every trial participants could look at the given product for as long as they wished. Participants could then assign the product to brand L or R by pressing the left and right arrow key, respectively. Depending on the correctness of their response, participants received feedback in form of a joyous, green smiley and the exclamation ''Correct!'' or in form of a sad, red smiley and the exclamation ''False!'' if their response was incorrect. Participants could continue looking at the product and the feedback for as long as they wished and then proceed to the next trial by pressing the upper arrow key. In case an invalid key was pressed, participants were reminded which keys to press to assign the product to a brand and to continue to the next trial. After the first 100 learning trials participants received every 50 trials feedback of their accuracy in the last 100 trials. Participants needed to achieve the accuracy criterion which consisted of correctly classifying each stimulus the last three occurrences and 80\% accuracy in the last 100 trials. If participants didn't meet the accuracy criterion within an hour, the experiment was discontinued and the participants received appropriate course credit. Else, participants continued with the second phase.

\subsubsection{Test phase}
Participants were reminded that the second phase didn't include feedback anymore and in case of time pressure were provided with the lower and upper integer of the exact time they had per trial. Without time pressure, participants faced no response deadlines. Participants were first informed that they will first perform some practice trials with already familiar products. Four blocks of the learning phase products were presented resulting in 32 familiarization trials. Upon completion, participants read that they should now assign further products to the two brands. Fourteen blocks consisting each of six novel and four familiar stimuli were presented resulting in 140 test trials. The sequence within each block was again randomized. The procedure in the test phase was equal to the one in the learning phase except that no feedback was provided, the following trial started 500 milliseconds after response entry, and a random half of participants had time pressure. If participants in the time pressure condition exceeded the time limit in a given trial they were informed that they were too slow and had to continue with the next trial after a 500 milliseconds inter--trial interval. After the test phase, participants filled out a questionnaire assessing key demographic variables (i.e., age, gender, education, and profession), vision (i.e., color blindness, impaired and corrected vision), and task--related characteristics (i.e., clearness of task and strategy used in the second phase). Participant then received appropriate course credit which marked the end of the experimental session.
% in a randomized sequence within each of the four blocks consisting of the eight learning stimuli

\section{Results}
Data was collected from \textit{start date} till \textit{end date}. Out of 73 people that showed up, two did not complete the experiment as they didn't achieve the accuracy criterion within an hour. Two of the remaining 71 participants were used for pretest purposes and additional eight participants had to be excluded from analyses as they reported that the task was somewhat or absolutely unclear to them. No participant had to be excluded due to the reaction time criterion. This resulted in 61 participants (43 females,, $M_{age}$ = 24.13 years, $SD_{age}$ = 6.39 years, age range: 19--50 years)  used for statistics and data analysis, out of which 30  had time pressure in the test phase and 31 didn't. Figure \texit{ref} illustrates for the six novel stimuli in the test phase the responses of participants separately by time pressure condition as well as the predictions of the Minkowski and the discrete metric in both the multidimensional and the unidimensional version across all parameter combinations reaching the accuracy criterion.

For all statistical tests, an alpha level of .05 was used, except when multiple comparison corrections are indicated. Analyses included inferential tests at the aggregate level and cognitive modeling at the aggregate and individual levels.

\subsection{Inferential tests at the aggregate level}
A linear mixed model with logit link was fitted to the observed responses to the novel stimuli in the test phase using the lme4 R package. As the model did not converge in its preregistered form, we diminished the number of estimated parameters. Specifically, we took together the stimuli 221, 231, 321, 331 for whom the model predictions were similar and fitted a random intercept, but no random slope per participant instead of the preregistered random slope, no random intercept. The final model included the fixed effects time pressure condition, stimulus (with three levels), and the interaction thereof and has participant as random effect. The interaction between time pressure and stimulus, postulated by H1 and H2, was tested with relative AIC weights \texit{Wagenmakers & Farrell, 2004, p. 194} and was found necessary as the full model with the interaction term had $5.36 * 10^{36}$ higher AIC weights than a restricted model without the interaction term. A supplementary model comparison using ANOVA yielded as well a better fit of the model with (vs. without) the interaction term ($\chi^{2}(2)$ = 145.51, \textit{p} < .001). 

The following tests refer to the coefficients of the full model where category A is coded as 1. We preregistered to accept H1a, H1b, H1c, and H2a, respectively, in case of a change of algebraic sign or a significant one-sided contrast when comparing a particular stimulus of interest with the remaining novel stimuli of the test phase (see preregistration for further details). The alpha level for the statistical tests of the contrasts was corrected using Holm--Bonferroni correction \textit{Holm}. H1a (people use the discrete metric under time pressure) predicted that the coefficient of stimulus 100 was the only negative coefficient or significantly lower than the remaining coefficients given time pressure. H1b (people use a unidimensional Minkowski metric under time pressure) predicted that the coefficient of stimulus 003 differed from the remaining stimuli in one of the aforementioned ways, irrespective of the coefficients' direction. H1c predicted reverse directions of the coefficients' signs compared to H1a. Analyses of the logit least--square means for the time pressure condition (0.73 for stimulus 100, -1.24 for stimulus 003, and 0.30 for the four combined stimuli) yielded the following results: H1a was rejected as the coefficient of stimulus 100 was positive and significantly higher than the remaining coefficients (\textit{p} < .001). H1b was accepted as the coefficient of stimulus 003 was significantly lower than the remaining coefficients (\textit{p} < .001). H1c was accepted due to the significant contrast between stimulus 100 and the remaining stimuli. These results thus support the use of a Minkowski metric on the aggregate level given time pressure, with the unidimensional version fulfilling both acceptance criteria and the multidimensional version fulfilling a significant contrast. H2a (people use the multidimensional Minkowski metric without time pressure) predicted as H1c the reverse coefficient directions than H1a, but this time for the data without time pressure. Analyses of the logit least--square means for the no time pressure condition (2.52 for stimulus 100, -3.02 for stimulus 003, and 0.52 for the four combined stimuli) yielded: H2a was accepted as the coefficient of stimulus 100 was significantly higher than the remaining coefficients (\textit{p} < .001). These results thus support the use of a multidimensional Minkowski metric on the aggregate level given no time pressure.

\subsection{Cognitive modeling}
To test H1d, H1e, H2b, and H2c, cognitive modeling was employed. During the modeling all fitting used maximum likelihood. Model parameters are as follows: The generalized context model with the multidimensional Minkowski and the multidimensional discrete metric have both five parameters (three attention weights \textit{w}s, the sensitivity parameter \textit{c}, and the softmax choice-rule parameter temperature $\tau$). The unidimensional model with Minkowski and the discrete metric have both three parameters (which of the attention weights \textit{w}s is set to 1, c, and $\tau$ defined as before).

The parameters of the generalized context model with the multidimensional Minkowski and the multidimensional discrete metric was fit to individual participants’ learning phase data. After fitting, the parameters of both model versions were fixed to the resulting optimal parameters for each participant. The attention weight parameter of the unidimensional models was fit to individual participants’ test phase data. To avoid over--fitting, the sensitivity parameter c and temperature τ of the unidimensional models were not fitted, but fixed to the corresponding multidimensional model’s parameter estimates for each participant. Figure \textit{ref} illustrates the distribution of fitted parameters across participants for the multidimensional and the unidimensional model versions. Participants put much attention to the second dimension, had very high sensitivity and very low temperature at the boundary of the valid range of the respective parameter which points towards many corner point solutions. For the unidimensional discrete model, 32 participants were best fit by the model with full attention on the first dimension, 29 by the model with full attention on the third dimension ($\chi^{2}(1)$ = 0.15, \textit{p} = .70). For the unidimensional Minkowski model, 21 participants were best fit by the model with full attention on the first dimension, 40 by the model with full attention on the third dimension ($\chi^{2}(1)$ = 5.92, \textit{p} = .01).

Log likelihoods of the observed test phase data were computed from the model’ predictions. We compared the multidimensional model with the discrete and the Minkowski metric, the unidimensional model with the discrete and the Minkowski metric, and a baseline random-choice model. If a unidimensional strategy attending to the second dimension described a participant best, we assumed they followed a random-choice model, because in our design these two models couldn't be distinguished. 

\subsubsection{Model comparison at the aggregate level}
For each model, using each participant's optimal parameters, the median log likelihood of the test phase data was computed separately for the condition with and without time pressure. The median log likelihoods were transformed into evidence strength (Akaike weights, Wagenmakers \& Farrell, 2004), and pairwise comparisons between models were conducted (as in Wagenmakers \& Farrell, 2004, p. 194). To determine the rank order of models, a model was accepted as superior to another model if its evidence ratio (normalized probability of one model over the other) in the paired comparison exceeded .90, rejected if it was inferior to .10; otherwise inconclusive evidence resulted. 

Using the data with time pressure, the rank order hypothesized in H1d (the multidimensional and unidimensional discrete metric on the first rank, the unidimensional Minkowski metric on the second rank, the multidimensional Minkowski metric on the third rank, and the random choice model on the final rank) couldn't be established. Specifically, the random model outperformed the remaining models, followed by the multidimensional discrete model on the second rank, the multidimensional Minkowski model on the third rank, and the unidimensional discrete model as well as the unidimensional Minkowski model on the final rank. 

Also without time pressure, the rank order hypothesized in H2b (the multidimensional Minkowski metric on the first rank and the random choice model on the last rank) couldn't be demonstrated. Specifically, the multidimensional discrete model exceeded the remaining models, followed by the multidimensional Minkowski model on the second rank, the random choice model on the third rank, the unidimensional Minkowski model on the forth rank, and the unidimensional discrete model on the final rank. Both H1d and H2b were thus rejected.

Table 1 shows the median log likelihood as well as mean and the standard deviation of the log likelihood, the mean absolute prediction error (MAPE), and the mean accuracy based on the arg max choice rule for the different models separately for the conditions with and without time pressure.

\begin{table}[ht]
\caption{Descriptive model fit measures}
\label{tab:model_fit_agg}
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{r|lllll|lllll}
  & \multicolumn{5}{l}{\emph{Time pressure}} & \multicolumn{5}{l}{\emph{No time pressure}} \\
  \hline
  Model & M(LL) & Md(LL) & SD(LL) & MAPE & arg max & M(LL) & Md(LL) & SD(LL) & MAPE & arg max \\ 
  \hline
  MDM & -115.53 & -110.25 & 28.52 & 0.47 & 0.51 & -84.58 & -79.60 & 29.85 & 0.38 & 0.63 \\ 
  MMM & -137.07 & -120.97 & 60.07 & 0.47 & 0.53 & -106.25 & -94.25 & 54.72 & 0.36 & 0.69 \\ 
  UDM & -160.29 & -150.89 & 66.27 & 0.42 & 0.51 & -181.95 & -151.16 & 83.10 & 0.46 & 0.52 \\ 
  UMM & -163.57 & -149.03 & 83.47 & 0.34 & 0.69 & -163.04 & -132.58 & 86.10 & 0.34 & 0.68 \\ 
  RAND & -89.79 & -93.23 & 7.76 & 0.50 & 0.00 & -97.04 & -97.04 & 0.00 & 0.50 & 0.00 \\ 
  \hline
\end{tabular}
}
\begin{tablenotes}
 \emph{Note:} Different fit measures are presented for the different models separately for the conditions with and without time pressure. Fit measures are the following: M(LL) = mean log likelihood, Md(LL) = median log likelihood, SD(LL) = standard deviation of log likelihood, MAPE = mean absolute percentage error, arg max = mean accuracy based on the arg max choice rule. Models are the following: MDM = multidimensional discrete metric, MMM = multidimensional Minkowski metric, UDM = unidimensional discrete metric, UMM = unidimensional Minkowski metric, RAND = random choice model. 
\end{tablenotes}
\end{table}

\subsubsection{Model comparison at the individual level}
For each model, using each participant's optimal parameters, the log likelihood of the test phase data was computed per participant. Individual log likelihoods were transformed into Akaike weights (Wagenmakers \& Farrell, 2004) and individual strategy classification was conducted on these Akaike weights. Specifically, if any model’s Akaike weight exceeded .90 the person was assigned to that model; else the person was classified as not described by any model. The resulting distribution of participants assigned to models is shown in Figure \textit{ref}.

With time pressure 17 participants were described by the random choice model, three were described by the multidimensional discrete metric and the unidimensional Minkowski metric, respectively, two were described by the multidimensional Minkowski metric, none was described by the unidimensional discrete metric, and five couldn't be described by any model. This observed distribution significantly differed from an expected equal distribution across models ($\chi^{2}(4)$ = 37.2, \textit{p} < .001). However, H1e (the multidimensional or unidimensional discrete model describes more people than any other model) was rejected, as the random choice model described far more people than any model using the discrete metric (Holm--Bonferroni corrected \textit{p} = .99). 

Without time pressure 15 participants were described by the multidimensional discrete metric, nine by the multidimensional Minkowski metric, three by the unidimensional Minkowski metric, two by the random choice model, none by the unidimensional discrete model, and two couldn't be described by any model. As in the time pressure condition, this observed distribution significantly differed from an expected equal distribution across models ($\chi^{2}(4)$ = 26.0, \textit{p} < .001). However, also H2c (the multidimensional Minkowski metric describes more people than any other model) was rejected, as the multidimensional discrete metric described six people more than the multidimensional Minkowski metric (Holm--Bonferroni corrected \textit{p} = 1).

The rank order of models was defined by the same pairwise comparison procedure using evidence ratios and the same acceptance and rejection criteria as described in ''model comparison at the aggregate level'' (see also Wagenmakers \& Farrell, 2004, p. 194). Using the data with time pressure, the in H1f hypothesized rank order of models (the multidimensional and unidimensional discrete model on the first rank, the unidimensional Minkowski model on the second rank, the multidimensional Minkowski model on the third rank, and the random choice model on the final rank) couldn't be established. Rather, the random choice model outperformed the remaining models, followed by the unidimensional Minkowski model on the second rank, the multidimensional discrete model on the third rank, the unidimensional discrete metric on the forth rank, and the multidimensional Minkowski model on the final rank. Similarly, using the data without time pressure, the in H2c hypothesized rank order of models (the multidimensional Minkowski model describes more people than any other model and the random choice model describes less people than any other model) couldn't be demonstrated. Rather, the multidimensional discrete model outperformed the remaining models, followed by the multidimensional Minkowski model on the second rank, the unidimensional Minkowski model on the third rank, the random choice model on the forth rank, and the unidimensional discrete model on the final rank. 

Finally, the equality of the distribution of the best-predicting models over participants between the conditions with and without time pressure was found significant using a Fisher's Exact Test (\textit{p} < .001). Pairwise post hoc comparisons (Holm-Bonferroni-corrected) yielded that H3a (the number of persons described by the multidimensional or the unidimensional discrete model is higher in the condition with than without time pressure) had to be rejected, given that more people were described by a discrete model without than with time pressure (15 and 3, respectively, \textit{p} = 1). Also, H3b (the number of persons described by the multidimensional Minkowski model is higher in the condition without than with time pressure) was rejected even though the multidimensional Minkowski model described nine people in the condition without time pressure, but only two people in the condition with time pressure (\textit{p} = .09).

\subsection{Explorative Analyses}

\subsubsection{Rule--based model}
A rule-based model was fit to the test phase data at the participant level, by means of a linear regression with features as predictors. As the test phase included 140 trials per participant, the complete test phase data fitted and predicted by the regression model in favor of a large data set to fit but also risking overfitting. In prediction, the fitted regression coefficients for every participants were used. Across participants, the mean estimated regression coefficients (with standard deviations in brackets) were -0.05 (0.18) for the first feature, 0.05 (0.11) for the second feature, -0.18 (0.14) for the third feature, and 0.73 (0.27) for the intercept which represents the stimulus 000. The mean R squared across all participants was .29 (SD = .19).

The rule-based model was compared to the other models using the model classification procedure by evidence weights detailed under ''model comparison at the individual level''. In the condition with time pressure, 28 people were described by the rule--based model, one person by the unidimensional Minkowski model, and one person couldn't be described by any of the models. In the condition without time pressure, 10 people were described by the linear model, eight by the multidimensional discrete model, seven by the multidimensional Minkowski model, and six couldn't be described by any of the models. Across both time pressure conditions, the rule--based model was thus the model that could describe the highest number of participants---however, the fact that the same data were used for fitting and predicting potentially overestimates the predictive capacity of the linear regression model. 
% When using a random half of the test phase data for each participant, the mean estimated regression coefficients (with standard deviations in brackets) turned out very similar with -0.05 (0.18) for the first feature, 0.06 (0.12) for the second feature, -0.18 (0.15) for the third feature, and 0.72 (0.29) for the intercept ($R^2$ = .32). The model comparison revealed that in the condition with time pressure 16 people were described by the rule--based model, two by the random choice model, one person by the unidimensional Minkowski model and the multidimensional Minkowski model, respectively, and ten people couldn't be described by any of the models. Without time pressure, 10 people were described by the rule--based model, nine by the multidimensional discrete model, six by the multidimensional Minkowski model, and six couldn't be described by any of the models. 

\subsubsection{The discrete threshold model}

\bibliography{example}

\end{document}

% KLEINSCHREIBUNG von Modellen (z.B. generalized context model anstatt Generalized Context Model)
% Questions: do I have to use the template? Use Wilcoxon sign test to check whether observed rank orders differ from hypothesized ones? Table: Do differences between MAPE and M(LL) make sense? MINK-MULTI or MMM or GCM with blabla in figures, tables etc.? check threshold model. what about chisq.multcomp? what about fisher.bintest? Ask again about regression model (half data).


%
% Please see the package documentation for more information
% on the APA6 document class:
%
% http://www.ctan.org/pkg/apa6
%
