\documentclass[a4paper,man,natbib]{apa6}

\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage{graphicx}
\usepackage{graphics}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage{amsmath,amsfonts,mathabx}
\usepackage{epigraph}
\usepackage[figuresright]{rotating}
%\usepackage[ansinew]{inputenc}
\usepackage{multirow}
\usepackage{nameref}
\usepackage{hyperref}
\usepackage{makecell}
\renewcommand{\cellalign/theadalign}{cl}
%\setmainfont{Times New Roman} 
%\setsansfont{Times New Roman} 
%\setmonofont{Times New Roman} 
% Keywords command

\newcommand{\cmmnt}[1]{\ignorespaces}

\providecommand{\keywords}[1]
{
  \textbf{\textit{Keywords---}} #1
}
\usepackage[doublespacing]{setspace}

\setlength\epigraphwidth{.8\textwidth}

\title{The Discrete Metric in Categorization Under Time Pressure}
% Effect of Time Pressure on Distance Metrics in Categorization
\shorttitle{Discrete Metric Under Time Pressure}
\author{Florian I. Seitz}
\affiliation{University of Basel}

\abstract{The generalized context model \citep{nosofsky1986attention}, which excels at explaining human categorization behavior, assumes the Minkowski metric to represent psychological distance. In the present pre--registered study, I tested the hypothesis that in categorization under time pressure people compare stimuli heuristically with the discrete metric which counts the number of non--identical feature values for a given stimulus pair. 
61 psychology students from the University of Basel took part in a categorization experiment that manipulated the existence of time pressure between--subjects. Participants first learned without time pressure the category membership of eight stimuli under trial--by--trial supervision. Then, participants categorized six novel and four old stimuli 14 times without feedback in the test phase. Time pressure was applied to a random half of participants in the test phase.
%61 psychology students from the University of Basel acquired the category structure of eight stimuli with three four--valued features  in the learning phase under trial--by--trial supervision. After meeting the accuracy criterion participants categorized six novel and four old stimuli 14 times without feedback in the test phase. I applied time pressure to a random half of participants in the test phase.
Inferential tests at the aggregate level showed that the model version with the Minkowski metric accounted for participants' categorization behavior both with and without time pressure ($p$ < .001, respectively). Computational cognitive modeling yielded that, without time pressure, the model with the discrete metric and attention on all features and, with time pressure, a random choice model outperformed the other models both on the aggregate and on the individual level.
The present findings suggest that under time pressure, people do not use the discrete metric. These findings imply that higher cognitive load does not reduce the complexity of psychological distance computation in categorization, but rather increases choice inconsistency. 
%We discuss related literature on choice inconsistency in preferential choice and also discuss to what extent a rule--based model could account for the findings.
} 

\keywords{categorization, time pressure, Minkowski metric, discrete metric, psychological distance}

\begin{document}
\maketitle

%\vspace*{\fill}
\epigraph{There is nothing more basic than categorization to our thought, perception, action, and speech. [...] An understanding of how we categorize is central to any understanding of how we think and how we function, and therefore central to an understanding of what makes us human.}{George Lakoff, 1987, pp.5--6}

Humans frequently engage and excel in categorization---the generation and identification of groups from a bigger amount of objects. Categorization is thus considered one of the most fundamental cognitive phenomena overall \citep{ashby2001categorization, bruner1956study, cohen2005bridging, lakoff1987women, goldstone2003concepts}. Past research has shed much light on the cognitive processes during categorization learning and generalization \citep[for an overview over the diverse cognitive models of categorization, see][]{kruschke2008models,wills2013models}. One finding from these studies is that people first rely on simple categorization strategies and only if necessary pass on to more complex strategies \citep{gluck2002people, meeter2006strategies, meeter2008probabilistic, johansen2002there, smith1998prototypes}. Simple categorization processes include the reduction of the number of attended features (resulting in an extreme case in a unidimensional classification process where categorization behavior depends only on one single feature ; \citealp{johansen2002there}) and by the use of a central tendency (i.e., a prototype) instead of multiple actual instances to represent a category \citep{smith1998prototypes}. 
%Both unidimensional strategies and prototype--based strategies have been found to account for human behavior during early categorization when people rely on simple strategies \citep{johansen2002there, smith1998prototypes}.

While unidimensional strategies and prototype--based strategies reduce the amount of information that enters the categorization process, another, remarkably different way to simplify the categorization process lies in simplifying the computation itself. This kind of simplification is the focus of the current study: Within the framework of the generalized context model \citep{nosofsky1986attention}, which bases the classification of an object on its similarity to category instances, I investigate a heuristic at the level of the psychological similarity function.
More specifically, I examine a computationally simple object comparison that checks the identity of two objects' values on a given feature and thus leads to binary feature differences. The computationally simple categorization process bases psychological similarity on these binary feature differences, whereas the more extensive categorization process base psychological similarity on metric feature differences. The present thesis tests this hypothesis in a categorization task under time pressure with an experimental design that optimally discriminates between the metric and binary difference computation while allowing as well to test the alternative strategy simplification of unidimensional strategies. 
%All strategies are implemented in the popular generalized context model \citep{nosofsky1984choice, nosofsky1986attention, nosofsky2011generalized} which is an exemplar--based model, where people retrieve instances from memory (i.e., exemplars) and compare them with an instance to categorize (i.e., the probe) to assess category membership \citep{medin1978context}. 

%Strategy simplification, however, has not been central in research on categorization under time pressure, albeit that in lack of time the need for simplifying the categorization process might be particularly pronounced. In contrast, research on the temporal aspects of categorization has focused on sequential information sampling such as competing retrieval of comparison objects from memory \citep{nosofsky1997exemplar, nosofsky1999effects, nosofsky2005speeded} or sequential processing of features (\citealp{lamberts1995categorization, lamberts1998time, lamberts2000information}; for a combination of both processes, see \citealp{cohen2003extension}). 

\subsection{Theoretical Background: Categorization under Time Pressure}
Categorizations often have to be carried out quickly. Imagine an emergency medical physician who has to quickly make a diagnosis in order to provide her patient with the right treatment or a traveler in the woods who has to decide whether a long, brown, and slightly curved object is a dangerous snake or a harmless branch. Fast categorization is required in both situations in order to prevent a negative outcome that results from delayed reaction. Empirical research, however, has only little investigated how people categorize objects under time pressure. 

One line of research attempts to uncover the temporal development of the categorization process \citep{lamberts2000information, lamberts2002feature, nosofsky1997exemplar}. Studies from this research line analyzed people's response times in perceptual categorization and modeled how people sequentially include information and adapt their categorization probabilities. One model that arose in this research is the exemplar--based random--walk model \citep{nosofsky1997exemplar} which assumes a sequential retrieval of exemplars from memory. Specifically, the exemplars race against each other for comparison with the probe. The similarity between the probe and every exemplars than won the race is computed and category membership probability is updated accordingly. Once a benchmark in favor of one of the categories is passed, people are assumed to render their response. Response times are hence dependent on the speed with which the different exemplars race, which is in turn a function of the exemplar's presentation frequency, presentation recency, and similarity to the probe. A second model of the temporal development is the extended generalized context model for response times \citep{lamberts2000information, lamberts2002feature} which assumes that people sequentially process the probe's features and thus serially construct a representation of the probe. The processed features of the probe are compared to the corresponding features of all stored exemplars in memory. The extended generalized context model for response times predicts response times to be dependent on how much stimulus information needs to be sampled until a categorization response can be rendered with sufficient confidence.

Both these approaches thus model response times by means of sequential information sampling and do not focus on the effect of response deadlines on categorization. However, the models' assumptions may be applied to categorization under time pressure and, in fact, the extended generalized context model for response times is derived from the extended generalized context model \citep{lamberts1995categorization}, which addresses just this topic. Assuming the same principle of sequential sampling of stimulus information, the extended generalized context model was shown to account for categorization under time pressure in a series of studies \citep{lamberts1995categorization, lamberts1998time, lamberts1999building, lamberts1999categorization, lamberts1997fast}. 
The majority of these studies analyzed the effect of time pressure on categorization in binary environments (i.e., where each feature contains only two possible values; \citealp{lamberts1995categorization, lamberts1998time, lamberts1999building, lamberts1999categorization}). In such binary environments, feature comparisons between stimuli cannot be simplified as two stimuli can differ on a given feature by maximally 1, which is the smallest amount of differences possible in order for a feature to contain any relevant information. The results from these studies are hence uninformative for the research question of this thesis, namely, whether people under time pressure simplify stimulus feature comparison. 

One study \citep{lamberts1997fast}, however, used stimuli with multivalued features and found that the extended generalized context model can account for categorization behavior in two experiments with time limits of 400 ms and 700 ms per trial. An alternative explanation to the findings of \cite{lamberts1995categorization} might be that participants simplify the feature comparisons between probe and exemplars when they face time pressure. Specifically, people may not compute metric feature differences between probe and exemplars under time pressure, but rely on a simple binary feature comparison which only checks for identity on a given feature. The metric feature comparison between probe and exemplars may be implemented by the Minkowski distance (see below for a formal description) which is, to my knowledge, the only measure of distance used in categorization models. The heuristic feature comparison that checks for identity of feature values is formalized by the discrete distance (see below for a formal description) and might replace the Minkowski metric in categorization under time pressure. In what follows, I will describe one of the most prominent models of categorization---the generalized context model \citep{nosofsky1986attention}---which assumes the Minkowski distance for comparisons between probe and exemplars and present an alternative version of the model that uses the discrete distance instead.

\subsection{Formal Models: The Generalized Context Model With two Distance Metrics}
One of the most prominent formal models for human categorization is the generalized context model \citep{nosofsky1984choice, nosofsky1986attention, nosofsky2011generalized} which is an exemplar--based model, where people retrieve instances from memory (i.e., exemplars) and compare them with an instance to categorize \citep[i.e., the probe;][]{medin1978context}. People are assumed to represent each exemplar as a point in a multidimensional space where the exemplar's feature values are the point's coordinates. Comparisons between probe and exemplars are expressed as distances, which can be interpreted as how far away the probe is spatially positioned from the exemplar of comparison. Distances are transformed into similarities, such that high distances correspond to low similarities and low distances to high similarities. Finally, the aggregate similarity of the probe to all exemplars of one category relative to the aggregate similarity to all exemplars states the model's prediction of category membership for the probe. The higher the similarity of the probe to the exemplars of one category the higher the model's prediction of assigning the probe into this very category. 

In the following, I will describe the three computational steps of distance, similarity, and categorization probability which the generalized context model assumes in a multidimensional, multivalued stimulus space with two categories. The formalization of the model is equivalent to \cite[][pp.281--282]{nosofsky1989further}, except that it has been generalized to more than two dimensions. 

\subsubsection{Categorization probability}
The generalized context model \citep{nosofsky1989further} predicts a probe's category membership by means of the similarity between the probe and the categories to choose from. Specifically, the probability with which probe $i$ is categorized into category $A$ is defined as 
\begin{equation}
P(R_{A}|i) = \frac{b_{A}\sum\limits_{j \in A} s_{ij}}{b_{A}\sum\limits_{j \in A} s_{ij} + (1 - b_{A})\sum\limits_{k \in B} s_{ik}},
\label{eq:probability}
\end{equation}
where $P(R_{A}|i)$ is the probability of assigning probe $i$ to category $A$, $s_{ij}$ is the similarity between probe $i$ and exemplar $j$, and $b_{A}$ is the response bias for category $A$ (with $0 \leq b_{A} \leq 1$). The function assumes probabilistic categorization in accordance with the relative aggregate similarity that is attributable to one of the two categories. The function is often referred to as Luce's choice axiom \citep{luce1959individual} and is derived from the similarity--choice model for stimulus identification \citep{luce1963detection, shepard1957stimulus}. 

\subsubsection{Similarity}
The similarity $s_{ij}$ between probe $i$ and exemplar $j$, which is needed for categorization probability, is itself computed from the distance between $i$ and $j$ in a multidimensional space. The transformation of distance into similarity is given by Shepard's universal law of generalization \citep{shepard1987toward}
\begin{equation}
s_{ij} = \exp\left(-c*d_{ij}^p\right),
\label{eq:similarity}
\end{equation}
where $d_{ij}$ is the distance between probe $i$ and exemplar $j$, $c$ (with $0 \leq c$) is an overall sensitivity parameter, and $p$ is a parameter that determines how similarity relates to psychological distance. Popular versions of the similarity function are the exponential decay function ($p = 1$) for discriminable stimuli and the Gaussian function ($p = 2$) for confusable stimuli \citep{ennis1988confusable, nosofsky1985luce}. The sensitivity parameter $c$ describes the convexity of the similarity function indicating thus the steepness with which similarity decreases at small distances. For high values of $c$, similarities are already low at small distances. For low values of $c$, similarities are still high at large distances. In both cases, the model only weakly discriminates between different sizes of distance. The parameter $c$ thus denotes a person's sensitivity to psychological distance.

\subsubsection{Distance}
As a requisite to assess the similarity, the generalized context model computes distances between probe and exemplars in a multidimensional space. A vast number of distance metrics exist that could represent the psychological distance of two objects \citep{deza2009encyclopedia}. In fact, the only prerequisites that geometric distance metrics need to meet are (a) the non--negativity axiom (all distances are greater than or equal to 0), (b) the identity of indiscernibles (distances between identical objects are 0), (c) symmetry (the distance between two objects is independent of the order of the objects), and (d) the triangle inequality (the distance between two objects is at least as small as the distance of these two objects via a third object; \citealp{restle1959metric}). The generalized context model and related models of categorization assume the Minkowski distance. I propose that under time pressure people use the discrete distance.

\paragraph{Minkowski distance}
The Minkowski distance is formally implemented in the generalized context model as
\begin{equation}
d_{ij} = \left[\sum\limits_{m=1}^M w_{m}*\mid x_{im} - x_{jm}\mid ^r\right]^\frac{1}{r},
\end{equation}
where $d_{ij}$ is the distance between probe $i$ and exemplar $j$, $x_{im}$ is the value of probe $i$ on feature $m$, $w_{m}$ is the attention weight attributed to feature $m$ (with $0 \leq w_{m} \leq 1$ and $\sum w_{m} = 1$ \footnote{Note that the attention weights $w$s are a psychological extension of the Minkowski metric that represent differential allocation of attention across features. In its mathematical definition the Minkowski distance does not include attention weights.}), $M$ is the number of features, and $r$ describes the form of the distance metric (with $r \geq 1$). Popular instances of the Minkowski distance are the Manhattan distance ($r = 1$) for highly separable features and the Euclidean distance ($r = 2$) for integral features \citep{shepard1964attention, nosofsky1986attention, garner1974processing}. Values of $r$ are constrained to be equal to or higher than 1 for adherence with the triangle inequality \citep{jakel2008similarity,francois2007concentration,tversky1982similarity,beals1968foundations}. For $r < 1$ the Minkowski distance between probe $i$ and exemplar $j$ may thus be larger than the summed Minkowski distance between probe $i$ and exemplar $k$ and between exemplar $k$ and exemplar $j$ and in such a case the Minkowski distance is not anymore a metric \citep[][p. 5]{kress1989linear}. 

\paragraph{Discrete distance}
The discrete distance is formally given by 
\begin{equation}
d_{ij} = \left[\sum\limits_{m=1}^M w_{m}* \rho_{m}(x_{im}, x_{jm}) ^r\right]^\frac{1}{r},
\label{eq:distance}
\end{equation}

where $\rho_{m}(x_{im}, x_{jm})$ is the discrete distance function that checks whether probe $i$ and exemplar $j$ have an identical value on dimension $m$. Checking the identity of feature values can result in a distance of 1, if probe $i$ and exemplar $j$ are not identical on dimension $m$, or in a distance of 0, if they are identical. In the generalized context model, this yields

\begin{equation}
\rho_{m}(x_{im}, x_{jm}) = 
\begin{cases}
	1 & x_{im} \neq x_{jm} \\
	0 & else 
\end{cases}.
\end{equation}

All parameters of the generalized context model are preserved in the version with the discrete distance; the only difference to the model version with the Minkowski metric lies in the way how distances between objects are computed.

\subsubsection{Relation to the unidimensional generalized context model}
Instead of using the heuristical discrete metric to simplify categorization under time pressure, people might as well base their categorization behavior on fewer stimulus features. In an extreme case, participants only attend to one feature, probably the feature they deem most relevant, and categorize a probe only based on this single feature. In the generalized context model \citep{nosofsky1989further} attention partition across features is given by the attention weights $w$s. Whereas in the multidimensional generalized context model attention weights are only constrained to be positive and to sum up to 1, in the unidimensional generalized context model one of the attention weights is set to 1 and the remaining to 0. In a given environment, there exist as many unidimensional generalized context model versions as the stimuli contain features. An important distinction between the unidimensional and the multidimensional generalized context model is that the unidimensional model version cannot model feature interactions. 

In the present study, people had to learn a non--linearly separable category structure which requires people to attend to multiple features. In other words, the unidimensional generalized context model cannot learn the category structure of the present study. As the unidimensional generalized context model fails to reach sufficient categorization accuracy during learning, I hypothesize that people under time pressure lower their computational load by means of a simple psychological distance computation (i.e., the discrete metric) instead of using the unidimensional generalized context model. The present thesis also investigates the unidimensional generalized context model with the discrete metric which needs the least computational power as both the distance metric is simplified and less features are attended to. In case this model describes a participant this is counted as evidence for the discrete metric as the primary aim of this thesis is to analyze whether people use different distance metrics depending on the amount of cognitive load.
%in conditions with time pressure and without time pressure.
 
\subsubsection{Relation to prototype models}
Whereas the generalized context model bases categorization on the similarity between probe and exemplars (i.e., actual instances experienced in the past; \citealp{medin1978context, nosofsky1986attention}), prototype models base categorization on the similarity between probe and prototypes (i.e., abstract central tendencies; \citealp{posner1968genesis}). Hence, in comparison to the generalized context model, prototype models cannot depict influences from individual exemplars directly, but only indirectly via prototypes \citep{nosofsky2011generalized, nosofsky1992exemplars, medin1978context}.
% Due to the very fine--grained representation of categories using actual experienced instances, the categorization predictions of the generalized context model are sensitive to influences from individual exemplars---influences which are absent in prototype models \citep{nosofsky2011generalized, nosofsky1992exemplars, medin1978context}. 
%Following a long debate of whether people represent categories using prototypes or exemplars, research indicates that exemplar-based categorization models using a non--linear similarity rule, such as the generalized context model, outperform prototype models in explaining participants' categorization behavior (\citealp{nosofsky1992exemplars}, see also \citealp{scholkopf2002learning}). However, further evidence indicates that people shift from a prototype--based model during early categorization to a computationally more complex exemplar--based model as the number of experienced exemplars increases (\citealp{smith1998prototypes}, but for an alternative view, see \citealp{nosofsky2002exemplar}). 

On the computational level, prototype models require people to abstract a central tendency of a category. In turn, people do not have to retrieve the individual exemplars of the category and thus have to process less information during categorization. Prototype models depict thus a computationally simple categorization process that considers only summary category information.
Still, I did not integrate a prototype--based model in the present thesis, as design optimization revealed that no design simultaneously may recover and discriminate the two distance metrics (i.e., the Minkowski and the discrete metric) and the two types of category representation (i.e., prototypes and exemplars). Furthermore, in the present study, participants had to learn the category membership of only a few stimuli in a non--linearly separable category structure---two design properties for which literature suggests that people use exemplar--based strategies \citep{smith1998prototypes, smith2000thirty}. Hence, I assume for the categorization task of the present study that people use an exemplar--based model, namely the well--established generalized context model \citep{nosofsky1986attention}.

\subsection{Hypotheses}
The present pre--registered study investigates whether psychological distance is computed heuristically in categorization with limited cognitive resources. Psychological distance is operationalized in the present study within the framework of the generalized context model \citep{nosofsky1989further} with two different distance metrics---the Minkowski metric and the heuristic discrete metric. Allowing both model versions of the generalized context model to attend to several features (i.e., multidimensional model versions) or to only one feature (i.e., unidimensional model versions) further enables to examine the amount of stimulus information processed in categorization with limited cognitive resources. In the present study, limited cognitive resources are operationalized with time pressure, as time is a fundamental cognitive resource in decision--making. In the baseline condition without time pressure, I expect people to use the well--established multidimensional Minkowski metric; in the condition with time pressure I expect people to use the heuristic discrete metric (either unidimensional or multidimensional). Based on this main hypothesis, I pre--registered the following specific hypotheses which all presuppose the use of the generalized context model: 

Under time pressure, people use the discrete metric (H1a), else a unidimensional Minkowski metric (H1b), else the multidimensional Minkowski metric (H1c). Across participants, the multidimensional and the unidimensional discrete metric outperform the remaining models in predicting participant behavior, followed by the unidimensional Minkowski metric on the second rank, the multidimensional Minkowski metric on the third rank, and the random choice model on the final rank (H1d). At the individual level, the multidimensional and the unidimensional discrete metric describe more participants than any of the remaining models (H1e). Furthermore, at the individual level, the rank order of H1d is found as well in the count of participants best described by each model (H1f). 

Without time pressure, people use the multidimensional Minkowski metric (H2a). Across participants, the multidimensional Minkowski metric outperforms the remaining models, while the random choice model is inferior to all remaining models in predicting participant behavior (H2b). I have no hypotheses about the relative performance amongst the unidimensional Minkowski metric, the unidimensional discrete metric, and the multidimensional discrete metric. At the individual level, the multidimensional Minkowski metric describes more participants than any of the remaining models and the rank order of H2b is as well found in the count of participants best described by each model (H2c). 

Finally, the multidimensional discrete metric describe more participants with time pressure than without time pressure (H3a), whereas the multidimensional Minkowski metric describes more participants without time pressure than with time pressure (H3b).

The pre--registration of the present study is available on the Open Science Framework (OSF) website at \href{https://osf.io/94e6u/}{https://osf.io/94e6u/}.

\section{Method}

\subsection{Optimal Experimental Design}
I designed the experimental task using optimal experimental design \citep{myung2009optimal}. An optimal experimental design is defined as a design with ``the greatest likelihood of differentiating the models under consideration'' \cite[][p. 500]{myung2009optimal}. Being able to discriminate between the discrete metric and the Minkowski metric means that each model is associated with unique expected behavioral predictions in the experimental design. The models under consideration can thus be discriminated well by the data and can be associated more exclusively with a given participant, in the expectation. Furthermore, through the maximization of model prediction differences, design optimization increases model recovery (i.e., the best--fitting model is also the model underlying the data; however, each scientific model is only an approximation of the participant's cognitive model, see \citealp{myung2009optimal}). 
%The advantages of design optimization are thus two--fold: A participant's behavior can be associated more exclusively with one of the models under consideration and, given that the participant used one of the models under consideration, the probability is higher that this best--fitting model is the data--generating model. 
Design optimization maximizes thus the experiment's degree of informativeness and cost-effectiveness while keeping the necessary sample and trial size at a low level \citep{cavagnaro2009better, ouyang2016practical, raffert2012optimally, atkinson2007optimum, nelson2005finding}. 

The advantages of design optimization are most pronounced when the different models differ quantitatively instead of qualitatively as well as when several variables have to be considered simultaneously during the designing process of the experiment making thus a good design hardly visible to the naked eye \citep{myung2009optimal}. Both criteria apply to the present study.
The potential of optimal experimental design is shown in a reanalysis of the designs of the first two experiments of \cite{smith1998prototypes} which aimed at distinguishing the generalized context model \citep{nosofsky1986attention} from the multiplicative prototype model \citep{smith1998prototypes} by using six--dimensional binary stimuli \citep{myung2009optimal}. Even though the best experimental design used in \citeauthor{smith1998prototypes} had a high model recovery of 88.8\%, the optimal design that was possible yielded a model recovery rate of 96.3\%, minimizing thus the risk of a model fitting error by eight percent.  

In light of these insights, I ran simulations to find a categorization environment such that the discrete metric and the Minkowski metric implemented in the generalized context model can be optimally discriminated given the responses during the test phase, in the expectation.\footnote{In the first simulations I further wanted to discriminate the generalized context model from the multiplicative prototype model. However, as simultaneously discriminating the two models and the two distance metrics led to insufficient model recovery rates, I dropped the multiplicative prototype model from the remaining simulation analyses.} The simulations were chronologically iterated over (a) the design (i.e., all possible category structures with equal category base rates for all subsets of eight of 64 stimuli in the learning phase, under the constraint that learning stimuli differ from each other maximally by 1 unit per feature to ensure that the discrete and the Minkowski metric yield identical distances and thus none is favored by the learning phase design), (b) the true model (i.e., the generalized context model with either the Minkowski metric or the discrete metric), (c) the true parameter combination (i.e., attention weights $w$'s ranging from $0$ to $1$ in steps of $1/3$ and the overall sensitivity parameter $c$ ranging from $0.1$ to $4.1$ in steps of $1$), and (d) the fitting model (i.e., the generalized context model with either the Minkowski metric or the discrete metric).

For a given design, true model, and true parameter combination I simulated binary participant responses (i.e., category A or B) based on the true model's predictions for the learning set which was replicated 20 times and for the test set (i.e., the stimuli that were not part of the learning stimuli) which was replicated 10 times. Then, the free parameters (i.e., attention weights $w$s and the overall sensitivity parameter $c$) of both models were fit to the simulated data of the learning set excluding the first eight trials and fixed to the resulting optimal parameters for each participant. Next, predictions for the test set were computed with both fitted models and the log likelihood between the simulated participant responses under the true model and the predictions of each fitted model was calculated for the test phase data. This procedure allowed conducting model recoveries for every design across all permissible parameter combinations. The optimal design was the design for which the winning model recovered the true data--simulating model best across the permissible model parameters. In the optimal design, the learning criterion was met by both models when adding a softmax choice rule. From the 56 stimuli in the test phase of the best design, the six stimuli that discriminated models best were selected. Figure \ref{fig:environment} shows the stimuli in the resulting design.

\subsection{Materials and Design}
The experiment was conducted on \textit{computers; I need to describe them in more detail} and programmed in Expyriment \citep{krause2014expyriment} based on prior work by \cite{albrechtxxxunstacking}. Participants classified different stimuli into two categories by pressing the left and right arrow keys. The stimuli were based on the material of \cite{albrechtxxxunstacking}. Each stimulus consisted of three features represented by grey beams with feature values ranging from one to four and being represented by colored squares. Figure \ref{fig:material} illustrates the material. Participants learned the category structure of eight stimuli in the learning phase, and categorized four of them and six novel stimuli in the test phase (see Figure \ref{fig:environment}). Table \ref{tab:environment} shows the median model predictions for all stimuli of the learning set and the test set. Category labels, the key--label association, the color--feature association, and the visual mapping of features to positions on the screen were randomized across participants.

\begin{figure}[htbp]
\centering
\includegraphics[width = \textwidth]{fig_material.PNG}
\caption{Stimuli used in the experiment. Each grey beam illustrates a feature and the number of colored squares represents the feature value.}
\label{fig:material}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width = \textwidth]{fig_environment.png}
\caption{Learning stimuli (in black and white) including their category membership and test stimuli (in grey). Each axis of the graph represents one feature, and the coordinates of each sphere represent the feature values of the respective stimulus. Stimuli in the learning phase include all learning stimuli, stimuli in the test phase include all test stimuli as well as the learning stimuli 002, 012, 101, and 111.}
\label{fig:environment}
\end{figure}

\begin{sidewaystable}
\begin{center}
\begin{threeparttable}
\caption{Environment used and median model predictions}
\label{tab:environment}
\begin{tabular}{ccccccccccc}
\toprule
 &  & \multicolumn{4}{c}{Discrete} & \multicolumn{4}{c}{Minkowski} \\
\cmidrule(r){3-6} \cmidrule(r){7-10}
 &  & \multicolumn{1}{c}{Multidimensional} & \multicolumn{3}{c}{Unidimensional} & \multicolumn{1}{c}{Multidimensional} & \multicolumn{3}{c}{Unidimensional} \\
\cmidrule(r){4-6} \cmidrule(r){8-10}
Stimulus & \multicolumn{1}{c}{Category} &  & \multicolumn{1}{c}{$w$_1 = 1} & \multicolumn{1}{c}{$w$_2 = 1} & \multicolumn{1}{c}{$w$_3 = 1} &  & \multicolumn{1}{c}{$w$_1 = 1} & \multicolumn{1}{c}{$w$_2 = 1} & \multicolumn{1}{c}{$w$_3 = 1} & \multicolumn{1}{p{50mm}}{participant responses}\\
\midrule
\addlinespace
\multicolumn{2}{c}{\emph{Learning phase}} \\
\addlinespace
001 & B & 0.97 & 0.74 & 0.50 & 0.74 & 0.97 & 0.74 & 0.50 & 0.74 & 0.85\\
002\makebox[0pt][l]{$^{\ast}$} & A & 0.09 & 0.74 & 0.50 & 0.26 & 0.09 & 0.74 & 0.50 & 0.26 & 0.08\\
011 & B & 0.99 & 0.74 & 0.50 & 0.74 & 0.99 & 0.74 & 0.50 & 0.74 & 1.00\\
012\makebox[0pt][l]{$^{\ast}$} & B & 0.93 & 0.74 & 0.50 & 0.26 & 0.93 & 0.74 & 0.50 & 0.26 & 0.77\\
101\makebox[0pt][l]{$^{\ast}$} & B & 0.91 & 0.26 & 0.50 & 0.74 & 0.91 & 0.26 & 0.50 & 0.74 & 0.85\\
102 & A & 0.03 & 0.26 & 0.50 & 0.26 & 0.03 & 0.26 & 0.50 & 0.26 & 0.16\\
111\makebox[0pt][l]{$^{\ast}$} & A & 0.07 & 0.26 & 0.50 & 0.74 & 0.07 & 0.26 & 0.50 & 0.74 & 0.00\\
112 & A & 0.01 & 0.26 & 0.50 & 0.26 & 0.01 & 0.26 & 0.50 & 0.26 & 0.00\\
\midrule
\addlinespace
\multicolumn{2}{c}{\emph{Test phase}} \\
\addlinespace
003 & - & 0.63 & 0.74 & 0.50 & 0.50 & 0.09 & 0.74 & 0.50 & 0.26 & 0.86\\
100 & - & 0.37 & 0.26 & 0.50 & 0.50 & 0.91 & 0.26 & 0.50 & 0.74 & 0.07\\
221 & - & 0.85 & 0.50 & 0.50 & 0.74 & 0.07 & 0.26 & 0.50 & 0.74 & 0.54\\
231 & - & 0.85 & 0.50 & 0.50 & 0.74 & 0.07 & 0.26 & 0.50 & 0.74 & 0.86\\
321 & - & 0.85 & 0.50 & 0.50 & 0.74 & 0.07 & 0.26 & 0.50 & 0.74 & 0.67\\
331 & - & 0.85 & 0.50 & 0.50 & 0.74 & 0.07 & 0.26 & 0.50 & 0.74 & 0.50\\
\bottomrule
\addlinespace
\end{tabular}
\begin{tablenotes}[para]
\textit{Note.} Median model predictions for the stimuli used in the learning phase and the test phase. The four stimuli from the learning phase marked with an asterisk also appeared in the test phase. Participant responses are the mean responses for each participant and stimulus aggregated over participants using the median.
\end{tablenotes}
\end{threeparttable}
\end{center}
\end{sidewaystable}

Participants' main task was to categorize stimuli into two categories. The dependent variable was the categorization decision. The independent variable was the time pressure induced every second participant in the test phase. It allowed a participant 400 milliseconds plus 30\% of the median decision time needed by this participant across the final 100 learning trials. The study had thus a repeated--measures 2x1 between--subjects design. Further variables assessed were response times for each trial and the time limit if any. The stimuli were described to the participants as products, features as ingredients with the feature value showing the quantity of the ingredient, and the two categories were operationalized as brands (i.e., brand L and brand R). 

\subsection{Participants}
In total, 61 psychology students from the University of Basel (43 females, $M_{age}$ = 24.13 years, $SD_{age}$ = 6.39 years, age range: 19--50 years), recruited over an online platform of the Faculty of Psychology, completed the experiment from \textit{start date} until \textit{end date} in exchange for course credit. In the test phase, every second participant faced time pressure (n = 30); the remaining participants had no time pressure (n = 31).
%All participants were asked about color blindness, visual impairment, and in case of impaired vision whether they carried a visual aid during the experiment. 
Participants selected themselves into the sample and there were no inclusion criteria. Up to six participants could show up to the same experimental session. Ten additional participants showed up to the experiment but their data were not used for statistics and data analysis, as they failed to reach the accuracy criterion in the learning phase within an hour (n = 2) or reported that the task was somewhat or absolutely unclear to them (n = 8). Further, two participants were used for pretesting purposes. 
% No participant met the exclusion criterion of exceeding with time pressure the time limit in more than 50\% of the test trials for a given test stimulus or having without time pressure a log transformed reaction time more than three standard deviations below the mean of the log transformed reaction times of the learning phase in more than 50\% of the test trials for a given test stimulus.

Sample size was predetermined by a model--based power simulation \citep{gluth2019importance}. Across participants the true probability of choosing the Minkowski metric changed from 70\% to 30\% and the true probability of choosing the discrete metric changed from 30\% to 70\% when time pressure was introduced. For every participant, a random combination of parameters that met the accuracy criterion was sampled for the two model versions, respectively, and a mixing probability was sampled from a normal distribution truncated between 0 and 1 with the mean being equal to the share of participants using the discrete metric in the respective time pressure condition and SD =  .3. For any given test phase trial the mixing probability indicated how much the predictions of the discrete model version influenced the probabilities underlying simulated participant responses relatively to the predictions of the Minkowski model version. The log likelihood of the simulated responses was calculated for both model versions on the participant level and transformed into Akaike weights \citep{wagenmakers2004aic}. A given model was accepted if its Akaike weight exceeded .95. One--sided two proportion z--tests were calculated for each sample size comparing the proportion of accepted discrete model versions with time pressure and without time pressure. Figure \ref{fig:power} illustrates the proportion of significant results for every sample sizes across 1000 iterations. Given an aspired power of .8, a total sample size of N = 60 (n = 30 in each condition) was necessary to achieve 89\% power. The final sample included 31 participants without time pressure and 30 participants with time pressure.

\begin{figure}[htbp]
\centering
\includegraphics[width = \textwidth]{fig_power.png}
\caption{Power in dependence of sample size N. Power was estimated in a model--based power simulation where the true probability of using the Minkowski metric changed from 70\% to 30\% and the true probability of using the discrete metric changed from 30\% to 70\% when time pressure was introduced. To achieve the aspired power of .8, 60 participants (n=30 in each condition) were needed.}
\label{fig:power}
\end{figure}

\subsection{Procedure}
Participants were welcomed to the laboratory of the Center of Economic Psychology, seated within a cubicle, and provided with an informed consent which they could sign in case of agreement. Upon signature, the experimenter started the experiment and the participant was presented a series of instructions on the computer (for the exact instructions, see \textit{appendix, still need to do this}: Participants first read, that they had to learn to assign different products to two different brands (brand L and brand R), that each product consisted of the same three ingredients but differed from other products in the specific ingredient quantities. They then saw a randomly chosen product (see Figure \ref{fig:material} for an example), read that each grey beam represented one ingredient and the number of colored squares in the beams indicated the quantity of the respective ingredient. Participants had to explore all possible feature values by clicking on each feature at least ten times. Each click changed the quantity of the respective feature by one unit. Participants then read that in each trial their task was to correctly guess the brand of a randomly chosen product by pressing the left and right arrow keys. Next, participants read that in the first phase of the experiment they would learn the correct brand of each product by receiving feedback. After consistently assigning the products to the correct brand, they would in the second phase again assign products to brands, but without receiving feedback. Participants in the time pressure condition furthermore read that they would have a time limit for their response in the second phase which they should not exceed.

\subsubsection{Learning phase}
In the learning phase participants learned the category structure of eight products (see Figure \ref{fig:environment}). All blocks of the learning phase included each of the eight products; the sequence within each block was randomized. In every trial participants could assign the product to brand L or R by pressing the left and right arrow key, respectively, without any time limit. As feedback, participants were presented a happy, green smiley and the exclamation ''Richtig!'' (English: ''Correct!'') in case of a correct response and a sad, red smiley and the exclamation ''Falsch!'' (English: ''False!'') in case of a incorrect response. The product and the feedback remained visible until the participant proceeded to the next trial by pressing the upper arrow key. 
Invalid key presses led to a reminder, which keys were to press to assign the product to a brand and to continue with the next trial. 
Figure \ref{fig:timeline} illustrates two trials of the learning phase.

After the first 100 learning trials participants received every 50 trials feedback on their accuracy in the last 100 trials. To continue with the test phase, participants needed to correctly classify the last three occurrences of each stimulus and reach 80\% accuracy overall in the last 100 trials. If participants did not meet this accuracy criterion within an hour, the experiment was discontinued and the participants received appropriate course credit.

\begin{figure}[htbp]
\centering
\includegraphics[width = \textwidth]{fig_timeline.png}
\caption{Timeline of the learning phase. Participants could look at at a given product for as long as they wished (first and third image) and enter their response via the left and right arrow keys. In case of a correct response, a happy, green smiley as well as ''Richtig!'' (English: ''Correct!'') appeared (second image). In case of an incorrect response, a sad, red smiley as well as ''Falsch!'' (English: ''False!'') appeared (forth image). In both cases, people could continue to look at the feedback for as long as they wished before proceeding to the next trial via the upper arrow key.}
\label{fig:timeline}
\end{figure}

\subsubsection{Test phase}
Participants were reminded that the second phase did not include feedback anymore and in case of time pressure were provided with the lower and upper integer of the exact time they had per trial. Without time pressure, participants faced no response deadlines. Participants first performed 32 practice trials in which the products of the learning phase were presented in four blocks with a randomized sequence per block. After practice, 14 blocks consisting each of six novel and four familiar stimuli were presented resulting in 140 test trials (see Figure \ref{fig:environment}). The sequence within each block was again randomized. The procedure in the test phase was equal to the one in the learning phase except that no feedback was provided, the following trial started 500 milliseconds after response entry, and a every second participant had time pressure. If participants in the time pressure condition exceeded the time limit in a given trial they were informed that they were too slow and had to continue with the next trial after a 500 milliseconds inter--trial interval. After the test phase, participants filled out a questionnaire assessing key demographic variables (i.e., age, gender, education, and profession), vision (i.e., color blindness, impaired and corrected vision), and task--related characteristics (i.e., clearness of task and strategy used in the second phase). Participant then received appropriate course credit which marked the end of the experimental session.

\section{Results}
Figure \ref{fig:pred_obs_agg} illustrates for the six novel stimuli in the test phase the responses of participants separately for each time pressure condition as well as the predictions of the generalized context model with both the Minkowski and the discrete metric as well as the multidimensional and the unidimensional version across all parameter combinations reaching the accuracy criterion.

\begin{figure}[htbp]
\centering
\includegraphics[width = \textwidth]{fig_pred_obs_agg.png}
\caption{Predicted and observed responses for the novel stimuli of the test phase. Points and errorbars on the left denote the mean response and the standard error across participants for each stimulus separately for each time pressure condition. The four violins on the right denote model predictions for all parameter combinations reaching the accuracy criterion. The shapes within the violins denote the median model predictions.}
\label{fig:pred_obs_agg}
\end{figure}

For all statistical tests, an alpha level of .05 was used, except when multiple comparison corrections are indicated. Analyses included inferential tests at the aggregate level and cognitive modeling at the aggregate and individual levels. For all analyses the randomized category labels were derandomized and, based on Figure \ref{fig:environment}, category A was coded as 1 and category B as 0.

\subsection{Inferential tests at the aggregate level}
Participants needed some more trials to reach the accuracy criterion in the condition with time pressure ($M$ = 256.97, $Md$ = 189, $SD$ = 193.00) than in the condition without time pressure ($M$ = 214.58, $Md$ = 140, $SD$ = 151.86). At the end of the learning phase, participants reached similar accuracy in the conditions with time pressure ($M$ = .85, $Md$ = .84, $SD$ = .04) and without time pressure ($M$ = .86, $Md$ = .87, $SD$ = .04). In the 32 practice trials with the stimuli from the learning phase, the onset of time pressure was noticeable: Participants in the condition with time pressure were less accurate than at the end of the learning phase ($M$ = .65, $Md$ = .67, $SD$ = .11). In contrast, participants in the condition without time pressure further increased their accuracy ($M$ = .96, $Md$ = .97, $SD$ = .06). Reaction times in seconds in the learning phase were similar for the participants with time pressure ($M$ = 1.96, $Md$ = 1.80, $SD$ = 0.55) and without time pressure ($M$ = 1.89, $Md$ = 1.78, $SD$ = 0.54). In the test phase, time limits for the participants in the condition with time pressure were in general somewhat lower than 1 second ($M$ = 0.90, $Md$ = 0.87, $SD$ = 0.13). Congruent with the onset of time pressure, reaction times in seconds in the test phase including the practice trials were lower in the condition with time pressure ($M$ = 0.63, $Md$ = 0.64, $SD$ = 0.15 \footnote{Note that in the condition with time pressure only trials where the time limit was not exceeded were included.}) than in the condition without time pressure ($M$ = 1.83, $Md$ = 1.65, $SD$ = 0.72). 

A linear mixed model with logit link was fitted to the observed responses to the novel stimuli in the test phase using the lme4 R package. As the model did not converge in its preregistered form, I diminished the number of estimated parameters. Specifically, I aggregated the stimuli 221, 231, 321, and 331 for whom the model predictions were similar and fitted a random intercept, but no random slope per participant instead of the preregistered random slope, but no random intercept. In the final model the criterion was the participant response, the fixed effects were time pressure condition, stimulus (with three levels), and the interaction thereof, and a random intercept was fitted by participant. Table \ref{tab:estimates} shows the regression coefficients of the final model. The interaction between time pressure and stimulus, postulated by H1 and H2, was tested with relative AIC weights \citep[][p. 194]{wagenmakers2004aic} and was found necessary as the full model with the interaction term had $5.36 * 10^{36}$ higher AIC weights than a restricted model without the interaction term. A supplementary comparison by likelihood ratio test corroborated the superiority of the model with the interaction term ($\chi^{2}(2)$ = 145.51, $p$ < .001). 

\begin{center}
\begin{threeparttable}
\caption{Fixed Effects Estimates for the Linear Mixed Model with Logit Link}
\label{tab:estimates}
\begin{tabular*}{\textwidth}{c @{\extracolsep{\fill}} ccccc}
\toprule
\multicolumn{1}{c}{Parameter} & \multicolumn{1}{c}{Estimate} & \multicolumn{1}{c}{$SE$} & \multicolumn{1}{c}{$z$} & \multicolumn{1}{c}{$p$}\\
\midrule
\addlinespace
Intercept & -0.03 & 0.18 & -0.18 & 0.86\\
Time Pressure & -0.04 & 0.18 & -0.21 & 0.83\\
Stimulus 100 & 1.65 & 0.08 & 20.56 & <.001\\
Stimulus 003 & -2.10 & 0.09 & -24.28 & <.001\\
Time Pressure x Stimulus 100 & -0.86 & 0.08 & -10.70 & <.001\\
Time Pressure x Stimulus 003 & 0.93 & 0.09 & 10.81 & <.001\\
\bottomrule
\addlinespace
\end{tabular*}
\begin{tablenotes}[para]
\textit{Note.} Fixed effects estimates for the linear mixed model with logit link. Coefficients were calculated using sum--to--zero contrasts \citep{singmann2017introduction}. Hence, the coefficient for the intercept is the mean of all means and the coefficients for the fixed effects are the differences between the mean of all means and the mean of the respective effect.
\end{tablenotes}
\end{threeparttable}
\end{center}
\vspace{8mm}

\begin{sidewaystable}
\begin{center}
\begin{threeparttable}
\caption{Hypotheses}
\label{tab:estimates}
\begin{tabular*}{\textwidth}{lp{55mm}p{100mm}ll}
\toprule
\multicolumn{1}{l}{Index} & \multicolumn{1}{l}{Hypothesis} & \multicolumn{1}{l}{Prediction} & \multicolumn{1}{l}{$P(R_{A}|i)$} & \multicolumn{1}{l}{Betas}\\
\midrule
\addlinespace
\multicolumn{1}{l}{\emph{H1}} & \multicolumn{1}{l}{\emph{With time pressure, people use...}} \\
\addlinespace
H1a & MULTI-DISC or UNI-DISC, ... & stimulus 100: class ''B'', the others: class ''A'' & -0.18 & 0.86\\
\addlinespace
H1b & else UNI-MINK, ...  & stimulus 003: class ''A'', the others: class ''B'' ($w_1$ = 1) \newline stimulus 003: class ''B'', the others: class ''A'' ($w_3$ = 1) & -0.21 & 0.83\\
\addlinespace
H1c & else MULTI-MINK. & stimulus 100: class ''A'', the others: class ''B'' & 20.56 & <.001\\
\addlinespace
H1d & \multicolumn{2}{p{135mm}}{rank order of model fits at the aggregate level: \newline MULTI-DISC = UNI-DISC > UNI-MINK > MULTI-MINK > RANDOM} & -24.28 & <.001\\
\addlinespace
H1e & \multicolumn{2}{l}{MULTI-DISC and UNI-DISC describe the most participants} & -10.70 & <.001\\
\addlinespace
H1f & \multicolumn{2}{p{135mm}}{rank order of model fits at the individual level: \newline MULTI-DISC = UNI-DISC > UNI-MINK > MULTI-MINK > RANDOM} & 10.81 & <.001\\
\midrule
\multicolumn{1}{l}{\emph{H2}} & \multicolumn{1}{l}{\emph{Without time pressure, people use...}} \\
\addlinespace
H2a & MULTI-MINK. & stimulus 100: class ''A'', the others: class ''B'' & -0.18\\
\addlinespace
H2b & \multicolumn{2}{p{150mm}}{rank order of model fits at the aggregate level: \newline MULTI-MINK > {MUTLI-DISC, UNI-DISC, UNI-MINK} > RANDOM} & 0.09 & -24.28\\
\addlinespace
H2c & \multicolumn{2}{p{150mm}}{rank order of model fits at the individual level: \newline MULTI-MINK > {MUTLI-DISC, UNI-DISC, UNI-MINK} > RANDOM} & 0.09 & 10.81 \\
\midrule
\multicolumn{1}{l}{\emph{H3}} & \multicolumn{1}{l}{\emph{Across time pressure conditions}} \\
\addlinespace
H3a & \multicolumn{2}{p{150mm}}{more people use MULTI-DISC with than without time pressure} & 0.18 & -0.18\\
\addlinespace
H3b & \multicolumn{2}{p{150mm}}{more people use MULTI-MINK without than with time pressure} & 0.09 & -24.28\\
\bottomrule
\addlinespace
\end{tabular*}
\begin{tablenotes}[para]
\textit{Note.} Hypotheses analyzed in the present study. DISC and MINK denote the discrete metric and the Minkowski metric, respectively. MULTI and UNI denote multidimensional and unidimensional model versions of the generalized context model \citep{nosofsky1989further}, respectively. RANDOM = random choice model.
\end{tablenotes}
\end{threeparttable}
\end{center}
\end{sidewaystable}
\vspace{8mm}

The following tests refer to the coefficients of the full model. I preregistered to accept H1a, H1b, H1c, and H2a, respectively, in case the coefficient of a stimulus of interest differed from the coefficients of the remaining novel stimuli of the test phase by the algebraic sign in the correct direction or by a significant one-sided contrast. Tests of contrasts used the Holm--Bonferroni alpha--level correction \citep{holm1979simple}. H1a (people use the discrete metric under time pressure) predicted that stimulus 100 is rather classified into class ''B'' than class ''A'', whereas the remaining stimuli are rather classified into class ''A'' than class ''B'' (see Figure \ref{fig:pred_obs_agg}). However, the linear mixed model results show that stimulus 100 was mostly classified into class ''A'' with $b_{100}$ = 0.73 > $ b_{221,231,321,331}$ = 0.30 > $b_{003}$ = -1.24 ($b$s are logit least--square means and higher $b$s indicate more category ''A'' responses). The difference between $b_{100}$ and the remaining coefficients was reliable in a planned post--hoc contrast, $p$ < .001. H1b (people use a unidimensional Minkowski metric under time pressure) predicted stimulus 003 is classified into another class than the remaining stimuli (see Figure \ref{fig:pred_obs_agg}). If the first feature is attended to, stimulus 003 is classified rather into class ''A'' and the remaining stimuli into class ''B''. If the third feature is attended to, stimulus 003 is rather classified into class ''B'' and the remaining stimuli into class ''A''. The linear mixed model results showed that stimulus 003 was mostly classified into class ''B'' with the difference between $b_{003}$ and the remaining coefficients being reliable in a planned post--hoc contrast, $p$ < .001. H1c predicted the reverse of H1a, namely that stimulus 100 is rather classified into class ''A'' and the remaining stimuli into class ''B'' (see Figure \ref{fig:pred_obs_agg}). The linear mixed model results support these findings and, as shown in H1a, the difference between $b_{100}$ and the remaining coefficients was reliable in a planned post--hoc contrast, $p$ < .001. 

H2a (people use the multidimensional Minkowski metric without time pressure) predicted that stimulus 100 is rather classified into class ''A'' and the remaining stimuli into class ''B'' (see Figure \ref{fig:pred_obs_agg}). The linear mixed model results were in line with this prediction with $b_{100}$ = 2.52 > $ b_{221,231,321,331}$ = 0.52 > $b_{003}$ = -3.02 (again, higher $b$s indicate more category ''A'' responses). The difference between $b_{100}$ and the remaining coefficients was reliable in a planned post--hoc contrast, $p$ < .001. 

\subsection{Cognitive modeling}
To gain deeper insight into the cognitive processes underlying categorization, cognitive modeling was employed. During the modeling all fitting used maximum likelihood. Model parameters were as follows: The generalized context model with the multidimensional Minkowski metric and the multidimensional discrete metric had both five parameters (three attention weights $w$s, the sensitivity parameter $c$, and the softmax choice-rule parameter temperature $\tau$). The unidimensional model with Minkowski metric and the discrete metric had both three parameters (which attention weight $w$ is set to 1, $c$, and $\tau$ defined as before). The decay parameter $p$ and the exponent of the distance metric $r$ were fixed to 1 for all model versions, as the experiment used discriminable stimuli with separable features \citep{ennis1988confusable, nosofsky1985luce, shepard1964attention, garner1974processing}. 

The parameters of the generalized context model with the multidimensional Minkowski metric and the multidimensional discrete metric were fit to individual participants’ learning phase data. After fitting, the parameters of both model versions were fixed to the resulting optimal parameters for each participant. The attention weight parameter of the unidimensional models was fit to individual participants’ test phase data. To avoid over--fitting, the sensitivity parameter $c$ and temperature $\tau$ of the unidimensional models were not fitted, but fixed to the corresponding multidimensional model’s parameter estimates for each participant. The distribution of fitted parameters across participants is shown in Figure \ref{fig:par_multidim} for the multidimensional model and in Figure \ref{fig:par_unidim} for the unidimensional model versions. 

\begin{figure}[htbp]
\centering
\includegraphics[width = \textwidth]{fig_par_multidim.png}
\caption{Distribution of parameter estimates for the multidimensional generalized context model. Grey shaded points within the violins indicate individual parameter estimates, the large point within each violin indicates the median parameter estimate across participants.}
\label{fig:par_multidim}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width = \textwidth]{fig_par_unidim.png}
\caption{Distribution of attention weight estimates for the unidimensional generalized context model. Bars indicate separately for the Minkowski metric and the discrete metric how many people are best described by full attention to the first and the third feature, respectively. Full attention to the second feature was not included as this model could not be discriminated from a random choice model. Parameter estimates of $c$ and $\tau$ were taken from the participants' respective optimal parameter estimates in the multidimensional generalized context model.}
\label{fig:par_unidim}
\end{figure}

Participants centered much attention on the second dimension, had very high sensitivity and very low temperature at the boundary of the valid range of the respective parameters which points towards many corner point solutions. For the unidimensional discrete metric, 32 of the 61 participants were best fit by the model with full attention on the first dimension, the remaining 29 participants by the model with full attention on the third dimension ($\chi^{2}(1)$ = 0.15, $p$ = .70). For the unidimensional Minkowski metric, 21 of the 61 participants were best fit by the model with full attention on the first dimension, the remaining 40 participants by the model with full attention on the third dimension ($\chi^{2}(1)$ = 5.92, $p$ = .01).

In the following I compare the generalized context model with the multidimensional discrete metric, the multidimensional Minkowski metric, the unidimensional discrete metric and the unidimensional Minkowski metric. Every sensible model was expected to outperform a baseline random choice model. If a unidimensional strategy attending to the second dimension described a participant best, I assumed they followed the random choice model, because in my design these two models could not be distinguished. 

\subsubsection{Model comparison at the aggregate level} \label{sec:res_agg}
For each model, using each participant's optimal parameters, the median log likelihood of the observed test phase data (i.e., the hold--out data) was computed from the models' predictions separately for the condition with and without time pressure. The median log likelihoods were transformed into evidence strengths \citep[Akake weights,][]{wagenmakers2004aic}, and pairwise comparisons between models were conducted \citep[as in][p. 194]{wagenmakers2004aic}. To determine the rank order of models in predicting behavior across participants, a model was accepted as superior to another model if its evidence ratio \footnote{The evidence ratio is the normalized probability of one model over the other.} in the paired comparison exceeded .90, rejected if it was inferior to .10; otherwise inconclusive evidence resulted. 

I first present results with time pressure. H1d stated the following rank order of models in predicting behavior across participants: multidimensional discrete metric = unidimensional discrete metric > unidimensional Minkowski metric > multidimensional Minkowski metric > random choice model. The results contradict the prediction as the random model outperformed the remaining models. The following rank order (with median log likelihoods in brackets) was observed: random choice model (-93.23) > multidimensional discrete metric (-110.25) > multidimensional Minkowski metric (-120.97) > unidimensional Minkowski metric (-149.03) = unidimensional discrete metric (-150.89). H1d was thus rejected. Table 2 shows the median log likelihood as well as mean and the standard deviation of the log likelihood, the mean absolute prediction error ($MAPE$), the mean accuracy based on the arg max choice rule, and the mean--square error ($MSE$) for the different models. Figure \ref{fig:log_lik} further illustrates the negative log likelihoods of the test phase data for the different models. 

In the condition without time pressure, interestingly, participants were best described by a multidimensional discrete metric. This is contrary to hypothesis H2b which stated that the multidimensional Minkowski metric outperforms the remaining models in predicting participants' behavior on the aggregate level and goes against central categorization literature which assumes a Minkowski metric \citep{nosofsky1986attention, nosofsky1989further, smith1998prototypes, nosofsky1994rule, nosofsky1984choice}. As the stimuli from the learning phase differed from each other maximally by one value on each feature, both the discrete metric and the Minkowski metric lead to equal distances given a set of attention weights. In this case, the use of the discrete metric rather than the Minkowski metric can be seen as ecologically rational, as it exploits all information from the environment equally well as the Minkowski metric, but is more computationally more frugal \citep{todd2007environments}. More specifically, the following rank order (with median log likelihoods in brackets) was observed: multidimensional discrete metric (-79.60) > multidimensional Minkowski metric (-94.25) > random choice model (-97.04) > unidimensional Minkowski metric (-132.58) > unidimensional discrete metric (-151.16). H2b was thus rejected. For additional fit indices, see Table \ref{tab:fitmeasures}, and for an illustration of the negative log likelihoods, see Figure \ref{fig:log_lik}.

\begin{sidewaystable}
\begin{center}
\begin{threeparttable}
\caption{Descriptive model fit measures}
\label{tab:fitmeasures}
\begin{tabular}{llcccccccccccc}
\toprule
 &  & \multicolumn{6}{c}{Time pressure} & \multicolumn{4}{c}{No time pressure}\\
\cmidrule(r){3-8} \cmidrule(r){9-14}
 &  & \multicolumn{3}{c}{Log likelihood} & & & & \multicolumn{3}{c}{Log likelihood} & & & \\
\cmidrule(r){3-5} \cmidrule(r){9-11}
Dimensionality & \multicolumn{1}{l}{Metric} & \multicolumn{1}{c}{$M$} & \multicolumn{1}{c}{$Md$} & \multicolumn{1}{c}{$SD$} & \multicolumn{1}{c}{$MAPE$} & \multicolumn{1}{c}{$Arg max$} & \multicolumn{1}{c}{$MSE$} & \multicolumn{1}{c}{$M$} & \multicolumn{1}{c}{$Md$} & \multicolumn{1}{c}{$SD$} & \multicolumn{1}{c}{$MAPE$} & \multicolumn{1}{c}{$Arg max$} & \multicolumn{1}{c}{$MSE$}\\
\midrule
\addlinespace
\multicolumn{2}{l}{\emph{Generalized Context Model}} \\
\addlinespace
\multirow{2}{*}{Multidim} & Discrete & -115.53 & -110.25 & 28.52 & 0.47 & 0.51 & 0.31 & -84.58 & -79.60 & 29.85 & 0.38 & 0.63 & 0.21\\
\cmidrule(r){2-14}
 & Minkowski & -137.07 & -120.97 & 60.07 & 0.47 & 0.53 & 0.34 & -106.25 & -94.25 & 54.72 & 0.36 & 0.69 & 0.24\\
\cmidrule(r){1-14}
\multirow{2}{*}{Unidim} & Discrete & -160.29 & -150.89 & 66.27 & 0.42 & 0.51 & 0.32 & -181.95 & -151.16 & 83.10 & 0.46 & 0.52 & 0.34\\
\cmidrule(r){2-14}
 & Minkowski & -163.57 & -149.03 & 83.47 & 0.34 & 0.69 & 0.29 & -163.04 & -132.58 & 86.10 & 0.34 & 0.68 & 0.29\\
\cmidrule(r){1-14}
\multicolumn{2}{l}{\emph{Random Choice Model}} & -89.79 & -93.23 & 7.76 & 0.50 & 0.00 & 0.25 & -97.04 & -97.04 & 0.00 & 0.50 & 0.00 & 0.25\\
\bottomrule
\addlinespace
\end{tabular}
\begin{tablenotes}[para]
\textit{Note.} Different fit measures for the different models separately for the conditions with and without time pressure. 
          Fit measures are the following: $M$ = mean log likelihood, $Md$ = median log likelihood, $SD$ = standard deviation of log likelihood, $MAPE$ = mean absolute percentage error, $Arg max$ = mean accuracy based on the arg max choice rule, $MSE$ = mean--square error.
\end{tablenotes}
\end{threeparttable}
\end{center}
\end{sidewaystable}

\begin{figure}[htbp]
\centering
\includegraphics[width = \textwidth]{fig_log_lik.png}
\caption{Negative log likelihood of the test phase data given model predictions for both time pressure conditions. Lower values indicate a better fit of the model to the test phase data. Points illustrate the individual negative log likelihoods. DISC and MINK denote the discrete metric and the Minkowski metric, respectively. MULTI and UNI denote multidimensional and unidimensional model versions, respectively. RANDOM = random choice model.}
\label{fig:log_lik}
\end{figure}

\subsubsection{Model comparison at the individual level} \label{sec:res_ind}
While the aggregated analyses above test the performance of models across participants, additional insights can be gained from analyzing how many individual participants each model can best describe. Ultimately, model comparisons at the individual level allow to infer by which model most participants are best predicted. 
% In order to see how well the models describe the behavior of the individual participants and thus ultimately to shed light on the cognitive processes in categorization of individuals, I conducted model comparisons at the individual level. 
The following procedure was used: For each model and participant, using the respective participant's optimal parameters, the log likelihood of the test phase data (i.e., the hold--out data) was computed. Individual log likelihoods were transformed into Akaike weights \citep{wagenmakers2004aic} and individual strategy classification was conducted on these Akaike weights. Specifically, if any model’s Akaike weight exceeded .90 the person was assigned to that model; else the person was classified as not described by any model. Figure \ref{fig:model_selection} illustrates how many participants each model could best predict. Figure \ref{fig:aic} further shows for each participant the Akaike weights of the different models.

\begin{figure}[htbp]
\centering
\includegraphics[width = \textwidth]{fig_model_selection.png}
\caption{Number of participants best fit by the generalized context model versions separately for both time pressure conditions. DISC and MINK denote the discrete metric and the Minkowski metric, respectively. MULTI and UNI denote multidimensional and unidimensional model versions, respectively. RANDOM = random choice model.}
\label{fig:model_selection}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width = \textwidth]{fig_aic.png}
\caption{Akaike weights of the models for each participant. Each bar represents one participant. The Akaike weights of the different models are stacked upon each other per participant and always sum up to 1. DISC and MINK denote the discrete metric and the Minkowski metric, respectively. MULTI and UNI denote multidimensional and unidimensional model versions, respectively. RANDOM = random choice model.}
\label{fig:aic}
\end{figure}

With time pressure the majority of the 30 participants (n = 17) were described by the random choice model. In contrast, only few participants were described by the unidimensional Minkowski metric (n = 3), the multidimensional discrete metric (n = 3) and the multidimensional Minkowski metric (n = 2), and no participant was described by the unidimensional discrete metric. The 5 remaining participants could not be described by any model. This observed distribution of best--predicting models 
% This count of participants best predicted by the different models was not equally distributed.
significantly differed from an equal distribution ($\chi^{2}(4)$ = 37.2, $p$ < .001). However, contrary to H1e, which states that the multidimensional discrete metric and the unidimensional discrete metric describe more people than any other model, only few participants were best described by the multidimensional discrete metric or the unidimensional discrete metric (n = 5). The random choice model described more than three times as many people than any model using the discrete metric (Holm--Bonferroni corrected $p$ = .99). 

The rank order of models at the individual level was established in the same way as described for the aggregate level above \citep[see also][p. 194]{wagenmakers2004aic}. For the condition with time pressure, H1f stated the following rank order of models in predicting individual participant behavior: multidimensional discrete metric = unidimensional discrete metric > unidimensional Minkowski metric > multidimensional Minkowski metric > random choice model. Contrary to this prediction, the random choice model outperformed the remaining models. More specifically, the following rank order was observed: random choice model > unidimensional Minkowski metric > multidimensional discrete metric > unidimensional discrete metric > multidimensional Minkowski metric. Due to these results, both H1e and H1f were rejected.

Without time pressure almost half of the 31 participants (n = 15) were described by the multidimensional discrete metric and about a third by the multidimensional Minkowski metric (n = 9). Only few participants were described by the unidimensional Minkowski metric (n = 3), by the random choice model (n = 2), and no participant was described by the unidimensional discrete model. The 2 remaining participants could not be described by any model. Again, the count of participants best predicted by the different models was not equally distributed ($\chi^{2}(4)$ = 26.0, $p$ < .001). Interestingly and contrary to H2c, which holds that the multidimensional Minkowski metric describes more people than any other model, the multidimensional discrete metric described 6 people more than the multidimensional Minkowski metric (Holm--Bonferroni corrected $p$ = 1). 

Also the rank order of models at the individual level yielded that the multidimensional discrete metric outperformed the remaining models. This is in line with the findings from the aggregate model comparisons above and goes again against previous, influential categorization literature \citep{nosofsky1986attention, nosofsky1989further, smith1998prototypes, nosofsky1994rule, nosofsky1984choice}. This finding contrasts also with H2c which stated that the multidimensional Minkowski metric predicts the behavior of more individuals than any other model. Rather, in the data the following rank order was observed: multidimensional discrete metric > multidimensional Minkowski metric > unidimensional Minkowski metric > random choice model > unidimensional discrete metric. Due to these results, H2c was rejected.

Finally, I compared the distribution of best--predicting models between the conditions with time pressure and without time pressure to analyze whether some models predict more participants best in one of the two conditions. This was the case as showed a Fisher's Exact Test for Count Data ($p$ < .001). With time pressure the multidimensional discrete metric described only 3 of 30 people (10\%), but without time pressure it described 15 of 31 people (48.39\%) . This goes against H3a, which states that more people are described by the multidimensional discrete metric in the condition with time pressure than without time pressure. In a pairwise post hoc comparison, these proportions revealed to differ robustly, but in the opposite direction as hypothesized in H3a (Holm-Bonferroni-corrected $p$ = 1).
Furthermore, the multidimensional Minkowski metric described only 2 of 30 people (6.67\%) with time pressure, but 9 of 31 people (29.03\%) without time pressure. This difference in proportions is in line with H3b, which states that more people are described by the multidimensional Minkowski metric in the condition without time pressure than with time pressure. However, the proportions did not differ robustly in a pairwise post hoc comparison ($p$ = .08).

\subsubsection{Summary}
The inferential tests on the aggregate level suggested that people use the Minkowski metric independent of whether they face time pressure or not, which is partially in line with the hypotheses. As hypothesized, without time pressure participants' choices were in line with multidimensional Minkowski metric, which is the cognitively more complex metric than the discrete metric. However, with time pressure the test phase data were in line with the unidimensional Minkowski metric with attention on the third feature and with the multidimensional Minkowski metric, whereas I hypothesized that people use the multidimensional discrete metric. While these results suggest that people may use a unidimensional classification strategy  
%the unidimensional Minkowski metric with attention on the third feature fulfills both the acceptance criteria of inverse algebraic signs and a significant contrast, and the multidimensional version fulfills the acceptance criterion of a significant contrast. 

In contrast with the inferential tests and with my hypotheses, cognitive modeling yielded that the multidimensional discrete metric outperformed the remaining models without time pressure and a random choice model was best able to describe participants with time pressure. Due to the binary environment during learning (i.e., all stimuli of the learning phase differed by maximally one value on each feature) participants may have been incited to use the heuristic discrete metric instead of the cognitively more complex Minkowski metric, as both metrics process all information that is available in the environment. Psychological distances thus did not differ for the discrete metric and the Minkowski metric, given a set of attention weights, and in this case it was ecologically rational to use the heuristic discrete metric \citep{todd2007environments}. Instead of a change in the categorization process, the onset of time pressure seemed to have increased choice inconsistency which is in line with previous literature on the effect of diminished cognitive capacities on preferential choice \citep{olschewski2018taxing, burks2009cognitive}.

\subsection{Explorative Analyses}
In addition to the inferential tests and the cognitive modeling, I conducted a set of preregistered explorative analyses. These analyses included on the one hand a linear rule--based model to analyze whether a linear rule could describe the data. On the other hand, I implemented a superordinate version of the discrete metric, namely the discrete--threshold metric, which fits for every participant by how many values a feature must differ between two stimuli in order to be perceived as different.

\subsubsection{Linear rule--based model}
In contrast to exemplar--based models, such as the generalized context model \citep{nosofsky1986attention}, different theoretical approaches predict category membership based on a set of rules. Literature suggests that people often use rules in categorization \citep{restle1962selection, tom1968attention, rouder2006comparing} and theory sharply differs between rule--based and exemplar--based categorization (\citealp{rouder2006comparing}; although models that are exemplar--based and rule--based have been proposed such as ATRIUM \citealp{erickson1998rules} and COVIS \citealp{ashby2011covis}). 

In the present study, members of Category A (B) have predominantly a value of 0 (1) on the first feature and a value of 1 (2) on the third feature (see Figure \ref{fig:environment}). Consequently, a linear rule where people categorize stimuli with lower values on the first or third feature rather into Category A and stimuli with higher values on the first or third feature rather into Category B may be an appropriate categorization strategy. To test such a strategy, I fit a linear regression model with logit link and with features as predictors to the test phase data at the participant level. As the test phase included 140 trials per participant, the complete test phase data was fitted and predicted by the regression model in favor of a large data set to fit but also risking over--fitting. In prediction, the fitted regression coefficients for every participants were used. Across participants, the mean estimated regression coefficients for the three features ($b_1$. $b_2$, $b_3$) and the intercept ($b_0$) were $b_1$ = -0.05 ($SD$ = 0.18), $b_2$ = 0.05 ($SD$ = 0.11), $b_3$ = -0.18 ($SD$ = 0.14), and $b_0$ = 0.73 ($SD$ = 0.27) for the intercept which represents the stimulus 000. The mean $R^2$ across all participants was .29 (SD = .19).

To analyze how many participants the linear rule--based model described compared to the other models I used the model classification procedure by evidence weights detailed in the individual model comparison above. The results show that in the condition with time pressure, almost all 30 participants were described by the rule--based model (n = 28). Only 1 participant was described by the unidimensional Minkowski model, and 1 participant could not be described by any of the models. In the condition without time pressure, a substantial amount of the 31 participants could be described by the linear model (n = 10), by the multidimensional discrete model (n = 8), and by the multidimensional Minkowski model (n = 7). The remaining 6 participants could not be described by any of the models. Across both time pressure conditions, the linear rule--based model was thus the model that could describe the highest number of participants---however, the fact that the same data were used for fitting and predicting potentially overestimates the predictive capacity of the linear regression model. 

\subsubsection{Discrete--threshold metric}
The many participants with time pressure that were best described by the random choice model indicate that people categorize differently under cognitive load, but not in line with the discrete metric. However, the assumption of the discrete metric that all non--identical feature values are also perceived as being different might be too deterministic. Rather, it might be that people vary with respect to the difference two feature values need to have in order to be perceived as being not identical. 
%to further test whether people change the distance metric in categorization under time pressure
To test this idea, a superordinate version of the discrete metric (i.e., the discrete--threshold metric) was implemented. According to the discrete--threshold metric two features have a distance of 0 if they differ by maximally $\gamma$ and 1 otherwise with the threshold $\gamma$ being a free parameter of the model. In this particular case with four--valued features, $\gamma$ could take the values 0, 1, and 2. If $\gamma$ equals 0, the discrete-threshold metric corresponds to the discrete metric mentioned above. Formally, the discrete--threshold metric is defined as

\begin{equation}
\rho_{m}(x_{im}, x_{jm}) = 
\begin{cases}
	1 & \mid x_{im} - x_{jm} \mid > \gamma \\
	0 & else 
\end{cases},
\end{equation}

where $\rho_{m}(x_{im}, x_{jm})$ is the distance function that checks whether probe $i$ and exemplar $j$ differ by maximally $\gamma$ on dimension $m$. The reminder of the model version using the discrete--threshold metric is given by equations \ref{eq:probability}, \ref{eq:similarity}, and \ref{eq:distance}. 

As the rule--based model, the model with the discrete--threshold metric was fit to the test phase data on the participant level. Learning phase data could not be fit, as the stimuli during learning diverged on each feature my maximally 1 rendering the estimation of $\gamma$ impossible. Optimal parameters for each participant were used to compute model predictions and log likelihoods of the same test phase data which was used during fitting. In the condition with time pressure, the discrete--threshold metric fit the majority of the 30 participants best with $\gamma$ = 0 (which corresponds to the model with the discrete metric; n = 20, 66.67\%) and about equally many participants with $\gamma$ = 1 (n = 6, 20\%) and $\gamma$ = 2 (n = 4, 13.33\%). In the condition without time pressure, the discrete--threshold metric fit even more of the 31 participants best with $\gamma$ = 0 (n = 24, 77.42\%) and only few participants with $\gamma$ = 1 (n = 6, 19.35\%) and $\gamma$ = 2 (n = 1, 3.26\%). Relative to the condition without time pressure, more people best fit by a high $\gamma$ in the condition with time pressure suggesting that under time pressure the feature values of two stimuli must differ by a greater amount in order to be perceived as different. However, the differences between the time pressure conditions were not robust ($p$ = .37). In order to reduce the bias caused by fitting to and predicting the same data set, the model comparison analysis retook for all 44 participants best fit by the discrete--threshold metric with $\gamma$ = 0 the log likelihood from the equivalent model with the discrete metric that was fit to the learning phase. This reduced the influence of over--fitting in the results of the following model comparison analysis, although caution is still advisable when interpreting the results. As the discrete metric is nested in the discrete--threshold metric, model comparison did not include the multidimensional discrete metric nor the unidimensional discrete metric. 

On the aggregate level, using the procedure detailed in ''\nameref{sec:res_agg}'', the discrete--threshold model outperformed the remaining models in the condition without time pressure ($M(LL)$ = -79.09, $Md(LL)$ = -77.14, $SD(LL)$ = 25.50, $MAPE$ = 0.37, $argmax$ = .63, $MSE$ = 0.19).
%, followed by the multidimensional Minkowski model on the second rank, the random--choice model on the third rank, the unidimensional Minkowski model on the forth rank, and the unidimensional discrete model on the final rank. 
With time pressure, the discrete--threshold model scored on the second rank ($M(LL)$ = -101.78, $Md(LL)$ = -96.12, $SD(LL)$ = 28.54, $MAPE$ = 0.45, $argmax$ = .46, $MSE$ = 0.27) and was still outperformed by the random choice model. The rank order of the remaining models is the same as presented in ''\nameref{sec:res_agg}'' and model fit coefficients for the remaining models are shown in Table \ref{tab:fitmeasures}. 
% random, discrete--threshold, MMM, UDM = UMM

On the individual level, using the procedure detailed in ''\nameref{sec:res_ind}'', in the condition with time pressure each about a third of the 31 participants were best described by the random choice model (n = 11, 36.67\%) and by the discrete--threshold metric (n = 9, 30\%). A few participants were described by the unidimensional Minkowski metric (n = 3, 10\%), and some could not be described by any of the models (n = 7, 23.33\%). In the condition without time pressure almost half of the 31 participants were best described by the discrete--threshold metric (n = 14, 45.16\%) and a third by the multidimensional Minkowski metric (n = 10, 32.26\%). The remaining participants were described by the unidimensional Minkowski metric (n = 4, 12.90\%), by the random choice model (n = 2, 6.45\%), or could not be described by any of the models (n = 1, 3.26\%).  

\section{Discussion}
The present thesis examined in a categorization task whether people, given they use the generalized context model \citep{nosofsky1986attention}, apply the heuristic discrete metric instead of the more extensive Minkowski metric when put under time pressure. Evidence from inferential statistics was not in line with this hypothesis as both with time pressure and without time pressure a cognitive model using the Minkowski metric was able to predict participant behavior. Cognitive modeling, in contrast, yielded that the multidimensional discrete metric outperformed the multidimensional Minkowski metric in both time pressure conditions. Contrary to expectations, the multidimensional discrete metric excelled in the condition without time pressure, but was outperformed by a random choice model in the condition with time pressure. More people with time pressure than without time pressure were described by the discrete metric, which is against the hypotheses, and by the multidimensional Minkowski metric, which is in line with the hypotheses, but not statistically significant. In sum, these findings suggest that people do not use the discrete metric under time pressure, but rather the Minkowski metric (inferential tests) or respond randomly (cognitive modeling). Cognitive modeling, however, also showed that the discrete metric competes the Minkowski metric in describing participants in both time pressure conditions. 
%Different people may thus represent psychological distance differently independent of the time they have for categorization.

\subsection{Implications for theory and research}
The present results indicate that people do not use different distance metrics in conditions with time pressure and without time pressure. Rather, there is an increase of random categorization when people have reduced cognitive capacities due to time pressure. Similar findings stem from the domain of preferential choice, where cognitive load produced by a secondary task increased participants' inconsistency in preferences \citep{olschewski2018taxing, burks2009cognitive}. 
%Thus instead of yielding a preference shift as claimed in previous literature \citep{burks2009cognitive, benjamin2013behavioral, deck2015effect, schulz2014affect}, people behaved more randomly under high cognitive load. 

\cite{olschewski2018taxing} modeled choice inconsistency with a probit choice model, where the utility of any given option is variable, but people always choose the option with the momentarily highest utility, and the trembling hand error, where the utility of any given option is fixed, but people choose with a certain probability the option with the smaller utility. 
Similarly, choice inconsistency may be integrated into the generalized context model: The analogy of the probit choice model is that people with low cognitive capacities assign not consistently the correct category to the retrieved exemplars. Specifically, each exemplar which is compared to the probe is falsely remembered as a member of the opposite category with a certain probability $p$, which leads to a variance in remembered category membership that is mimicking the variability of utility in the preferential choice domain. The analogy of the trembling hand model lies to some extent in Luce's choice axiom of the generalized context model which states that people choose the less likely category with a probability equal to the model's predictions for this category. Note that Luce's choice axiom may be extended with a response--scaling parameter which tunes how deterministically responses are rendered \citep{nosofsky2002exemplar, nosofsky2011generalized, ashby1993relations}. One key difference to the trembling hand model, however, is that even in Luce's choice axiom with the response--scaling parameter the response probabilities are not stable within participants, but stimulus--dependent. Specifically, a stimulus $i$ with extreme categorization probabilities (e.g., $P(R_{A}|i)$ = .90) is classified less often into the less likely category than a stimulus with categorization probabilities close to random choice (e.g., $P(R_{A}|i)$ = .60). If one wanted to implement the trembling hand model in the generalized context model, then Luce's choice axiom needs to replaced by the $argmax$ choice rule, according to which people always choose the category with the highest probability of including the probe, and a trembling hand error, which is a participant--wise constant probability of assigning the probe to the less likely category. However, given that Luce's choice axiom accurately predicts participants' choice proportions \citep{nosofsky1987attention, mckinley1995investigations, lamberts2000information}, it is unclear whether this alternative choice rule may outperform Luce's choice axiom. 

In the present study, choice inconsistency was modeled with $softmax$, which makes categorization responses less deterministic with increasing temperature $\tau$. Fitting the test phase data exploratively showed that $\tau$ was higher under time pressure than without time pressure when using the Minkowski metric (with time pressure: $M$ = 0.38, $Md$ = 0.17, $SD$ = 0.45; without time pressure: $M$ = 0.14, $Md$ = 0.10, $SD$ = 0.09; $t$(31.05) = -2.80, $p$ = .004) and when using the discrete metric (with time pressure: $M$ = 0.98, $Md$ = 0.22, $SD$ = 2.48; without time pressure: $M$ = 0.13, $Md$ = 0.10, $SD$ = 0.07; $t$(29.05) = -1.88, $p$ = .03). Thus, independent of the metric used to fit $\tau$, the results support the insight that people respond more inconsistently under time pressure than without time pressure.

Given the increase of inconsistent responses under time pressure and cognitive load, further research should try to identify the exact causes of choice inconsistency in categorization (i.e., whether responses are inconsistently computed or executed), which is debated as well in other psychological domains \citep{blavatskyy2010models}. To that end, experimental design optimization \citep{myung2004model} could reveal which assumptions have to be met such that different models of stochastic choice can be discriminated from each other and which environments maximize discrimination and are thus potential candidates to test stochastic choice models against each other.

\subsection{Alternative cognitive processes: rule--based decision--making}
Parameter estimates in Figure \ref{fig:par_multidim} show that participants in the aggregate attended primarily to the second feature. However, in my design, the second feature alone was uninformative for participants, as both categories were equally distributed across the values of the second feature in the learning phase (see Figure \ref{fig:environment}). In other words, for both feature values of 0 and 1 on the second feature there were two members of category A and two members of category B such that knowing the value of the second feature gives by itself no information of category membership. In the generalized context model attending to the second feature thus shifts the predictions to the random choice level of .50 (see also Table \ref{tab:environment}). While a random choice model or the, in the experimental task, equivalent unidimensional generalized context model attending to the second dimension cannot achieve the accuracy criterion in the learning phase, a sequential rule--based model could potentially explain these results. 
%In fact, the second feature is only uninformative for geometric distance functions, however the value on the second feature can be exploited to decide which of the remaining features should be attended to in a next step. 
Specifically, given a value of 0 on the second feature, the third dimension discriminates the categories perfectly, and given a value of 1 on the second feature, the first dimension discriminates the categories perfectly. Instead of retrieving exemplars, participants could thus have learned the category structure by using a two--step rule--based model, where the first step is to always attend to the second feature and the second step is to attend to one of the remaining features contingent on the result from the first step.

Literature suggests that people use rules predominantly for well--defined categories \citep{restle1962selection, tom1968attention}, when the stimuli are confusable \citep{rouder2006comparing}, at the beginning of the categorization process \citep{rouder2006comparing}, and conjointly with an exemplar--based strategy such as in ATRIUM \citep{erickson1998rules} and in COVIS \citep{ashby2011covis}. While the categories in the present study were well--defined and the models were fit to the first phase of the experiment, the stimuli were discriminable rather than confusable, as feature values were indicated with colored unit squares. However, features differed visually only by the color of the squares such that people might have confused different features with each other. 
%Thus, participants may have confused learning stimuli such as 012 and 102 where equal feature values belong to different features. 
Past research thus does not exclude that people used a rule model in the present study. Furthermore the data of the test phase indicate that people without time pressure might have also used a rule to classify the novel stimuli as their responses were very deterministic in comparison to the participants with time pressure. Specifically, the variance of responses for the novel stimuli of the test phase calculated by stimulus and participant was higher with time pressure (aggregated over stimuli and participants: $M$ = 0.16, $Md$ = 0.18, $SD$ = 0.09) than without time pressure (aggregated over stimuli and participants: $M$ = 0.06, $Md$ = 0.00, $SD$ = 0.08). Possibly, participants without time pressure generalized the rule with which they learned the category structure in the learning phase to the novel stimuli from the test phase, whereas participants with time pressure lacked the temporal resources to execute the rule and behaved more randomly.

\subsection{Generalization to unfamiliar feature values}
The present results from the test phase further indicate that people might respond differently to novel stimuli with a higher or lower number of familiar feature values. Specifically, the two novel stimuli with two familiar feature values out of three were classified predominantly in line with the multidimensional Minkowski metric version of the generalized context model, whereas the four novel stimuli with one familiar feature value out of three were classified at least partially in line with the multidimensional discrete metric version of the generalized context model. Whereas this raises the possibility that people simplify their distance computation by using the discrete metric when facing novel stimuli with many unfamiliar feature values, past literature on generalization does not support this hypothesis, since the predictions of the discrete metric go into the opposite direction of the observed responses \citep{erickson2002rule, denton2008rule}.

In contrast, an extension of the rule--based model outlined above might account for why some novel stimuli of the test phase match the predictions of the multidimensional Minkowski metric, whereas others match the predictions of the multidimensional discrete metric: The rule--based model could first check whether the stimulus has a familiar value on the second feature. If so (i.e., for the stimuli 100 and 003), the model continues with the same two steps as mentioned above. If not (i.e., for the stimuli 221, 231, 321, and 331), the model checks whether one of the remaining features has a familiar value, which in this case would always be the third feature. Given that more members of category A have the same value on the third feature as the stimuli 221, 231, 321, and 331, the model may predict to classify them into category A. 

\subsection{Limitations}
The present study goes against the hypothesis that people compute distance heuristically under low cognitive capacities. Rather, participants responded more inconsistently under time pressure while in the condition with time pressure, interestingly, the multidimensional discrete metric competed with the multidimensional Minkowski metric in predicting participant behavior. An issue of the present study is however, that due to the between--subjects design it is unknown whether people switch their distance metric in dependence of the amount of time pressure. A within--subject design might have shed light on this issue, however this would have implied to set up two different stimulus environments, one for each time pressure condition. This procedure, in turn, might engender learning and training effects, is more extensive to conduct and is problematic, as the two stimulus environments probably do not discriminate equally well between the different models under consideration.
%between subjects design --> does one person use the same metric with / without time pressure

A second issue of the present design is that feature values are discrete integers and visually well perceptible. In such a case, using the Minkowski metric might not be cognitively demanding and as a result the advantage of the discrete metric concerning the cognitive simplicity relative to the Minkowski metric is diminished. In this sense, the present stimulus material provided a hard test for the discrete metric. In environments with continuous features where the calculation of metric distances is complicated the cognitive simplicity of the discrete metric relative to the Minkowski metric is more pronounced. In such cases, it is possible that people use the heuristic discrete metric more often under low cognitive capacities.
%stimulus material is a very hard test for the discrete metric

Finally, a third issue is that the learning environment has been established such that both the discrete metric and the Minkowski metric lead to equal distances. As already mentioned, in such an environment it is ecologically rational to use the heuristic discrete metric in the learning phase, as it exploits all available information from the environment \citep{todd2007environments}. In that case participants' response times in the learning phase reflect the duration of the processes in the generalized context model with the discrete metric. Establishing a time limit based on these response times thus might have rendered the use of the discrete metric in the time pressure condition computationally to extensive and rather choice consistency was reduced. In the condition without time pressure, participants could further use the discrete metric, which is in line with the data without time pressure, where 15 of 31 participants were best fit by the multidimensional discrete metric.

\subsection{Conclusion}
The present study analyzed whether people with low cognitive capacities simplify the categorization process by using the generalized context model \citep{nosofsky1986attention} with the heuristic discrete metric instead of the Minkowski metric. Results indicate that people do not change the distance function in dependence of the time available for categorization: Inferential statistics found that the Minkowski model version could predict responses for the test phase both with and without time pressure. Cognitive modeling found that the introduction of time pressure led to an increase of random categorization, which is in line with research on the effects of cognitive load on choice inconsistency \citep{olschewski2018taxing}. These results suggest that people do not adapt their strategy to the available cognitive capacities, but rather display cognitive overload which is manifested by choice inconsistency. Further categorization research may implement different existing models of stochastic choice \citep{blavatskyy2010models, becker1963stochastic} to analyze which cognitive processes underlie  randomness under low cognitive capacities.

\bibliography{example}

\end{document}

% KLEINSCHREIBUNG von Modellen (z.B. generalized context model anstatt Generalized Context Model)
% Questions: do I have to use the template? nope, should be okay, Jana checks this
% Use Wilcoxon sign test to check whether observed rank orders differ from hypothesized ones? nope
% Table: Do differences between MAPE and M(LL) make sense? could make sense as MAPE is severe when predictions and obs are small
% MINK-MULTI or MMM or GCM with blabla in figures, tables etc.? spell it out!
% check threshold model. what about chisq.multcomp? what about fisher.bintest? Ask again about regression model (half data). 
% Write discrete metric as formula? yes, perhaps begin with degree of belief (evidence strength) = probability for category 1, choice rule and go to distance computation; softmax am Ende der Modellierung --> for both model versions I use the soft max
% Discrete-threshold model fitted to learning phase? 
% without (vs. with) okay? don't use brackets, write with time pressure than without time pressure
% do simulation studies with the MPM have to be in thesis? no

% To do: look for start and end date, which computers were used
% Jäkel, Schölkopf, Wichman: Generalization and similarity 
% search: environment complexity --> metric changes? no feedback --> metric changes? consequences (false categorizations give minus points)
%
% Please see the package documentation for more information
% on the APA6 document class:
%
% http://www.ctan.org/pkg/apa6
%
